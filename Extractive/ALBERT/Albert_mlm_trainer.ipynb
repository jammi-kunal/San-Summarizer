{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ntd1jf7bzC9J"
   },
   "source": [
    "# \"Sanskrit Albert\"\n",
    "> \"Training a Language model from scratch on Sanskrit using the HuggingFace library, and how to train your own model too!\"\n",
    "\n",
    "- toc: true \n",
    "- comments: true\n",
    "- categories: [jupyter, NLP, HuggingFace]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XKVtBr98lMmL",
    "outputId": "2ae3e6af-a820-4a06-cd2f-c7819dd021a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr  3 11:48:37 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   71C    P8    33W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_ImQQX3BVfN"
   },
   "source": [
    "HuggingFace Recently updated their scripts, and the pip is yet to be released. So We'll build from source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kWMe_m6iIw5H",
    "outputId": "8f6e58a2-4f84-4d66-fd71-f7c7540dbbe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.5 MB 5.2 MB/s \n",
      "\u001b[?25hInstalling collected packages: tokenizers\n",
      "Successfully installed tokenizers-0.11.6\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-eppuIcNyWa",
    "outputId": "e0cd9fe6-0704-4d07-c96d-29267910b670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformers'...\n",
      "remote: Enumerating objects: 108324, done.\u001b[K\n",
      "remote: Total 108324 (delta 0), reused 0 (delta 0), pack-reused 108324\u001b[K\n",
      "Receiving objects: 100% (108324/108324), 94.69 MiB | 24.95 MiB/s, done.\n",
      "Resolving deltas: 100% (78900/78900), done.\n",
      "Processing ./transformers\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (21.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (4.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (2.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (3.6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "\u001b[K     |████████████████████████████████| 67 kB 3.0 MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 11.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (4.63.0)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     |████████████████████████████████| 596 kB 41.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (0.11.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (2019.12.20)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0.dev0) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.18.0.dev0) (3.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.18.0.dev0) (3.7.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (2021.10.8)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (7.1.2)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for transformers: filename=transformers-4.18.0.dev0-py3-none-any.whl size=3958150 sha256=8cce51fcca7a9f81eb20b88c4689edd653f91cc97771403535972b94035fe48a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-t68vl5vo/wheels/49/62/f4/6730819eed4e6468662b1519bf3bf46419b2335990c77f8767\n",
      "Successfully built transformers\n",
      "Installing collected packages: pyyaml, sacremoses, huggingface-hub, transformers\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 transformers-4.18.0.dev0\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers\n",
    "!pip install transformers/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5S9cL1otlalH"
   },
   "outputs": [],
   "source": [
    "!mkdir data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82QHQupvvsKX",
    "outputId": "ad2e2300-911b-4dfa-c9d9-31c8d244292c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |▎                               | 10 kB 17.6 MB/s eta 0:00:01\r",
      "\u001b[K     |▌                               | 20 kB 12.6 MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 30 kB 9.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 40 kB 5.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█▍                              | 51 kB 4.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 61 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 71 kB 5.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██▏                             | 81 kB 5.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 92 kB 6.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 102 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 112 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███▎                            | 122 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 133 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 143 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 153 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 163 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 174 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 184 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 194 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▍                          | 204 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 215 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 225 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 235 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▌                         | 245 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 256 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 266 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 276 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 286 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 296 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 307 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 317 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 327 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 337 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 348 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 358 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 368 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 378 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 389 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▌                     | 399 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 409 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 419 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 430 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 440 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 450 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 460 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 471 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 481 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 491 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 501 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 512 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 522 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 532 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▎                 | 542 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 552 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 563 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 573 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 583 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 593 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 604 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 614 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 624 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 634 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 645 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 655 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 665 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 675 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 686 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 696 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 706 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 716 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 727 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 737 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 747 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 757 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 768 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 778 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 788 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 798 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 808 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 819 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 829 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 839 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 849 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 860 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 870 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 880 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 890 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 901 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 911 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 921 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▋       | 931 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 942 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 952 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 962 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 972 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 983 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 993 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 1.0 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 1.0 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 1.0 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 1.0 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 1.0 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 1.1 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 1.1 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 1.1 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 1.1 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 1.1 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 1.1 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▍  | 1.1 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 1.1 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 1.1 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 1.1 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 1.2 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 1.2 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 1.2 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 1.2 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 1.2 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 1.2 MB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.2 MB 5.4 MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.96\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpLtcQ2K1C3H"
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from tokenizers import SentencePieceBPETokenizer, BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AJYPGD5v9Bvn",
    "outputId": "2aa3fcf6-cf6c-411e-8b72-5427ef5472f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-03 11:49:20--  https://github.com/Kartik-Bhatnagar/Sanskrit_Bert/raw/main/Corpus/Corpus.zip\n",
      "Resolving github.com (github.com)... 140.82.113.3\n",
      "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/Kartik-Bhatnagar/Sanskrit_Bert/main/Corpus/Corpus.zip [following]\n",
      "--2022-04-03 11:49:21--  https://raw.githubusercontent.com/Kartik-Bhatnagar/Sanskrit_Bert/main/Corpus/Corpus.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7399338 (7.1M) [application/zip]\n",
      "Saving to: ‘Corpus.zip’\n",
      "\n",
      "Corpus.zip          100%[===================>]   7.06M  --.-KB/s    in 0.08s   \n",
      "\n",
      "2022-04-03 11:49:21 (84.0 MB/s) - ‘Corpus.zip’ saved [7399338/7399338]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://github.com/Kartik-Bhatnagar/Sanskrit_Bert/raw/main/Corpus/Corpus.zip'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nl2fZjTr-3zv",
    "outputId": "894b95a0-b0a0-4dee-cd83-584ae94903d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/Corpus.zip\n",
      "   creating: Corpus/\n",
      "  inflating: Corpus/file_train2.txt  \n",
      "  inflating: Corpus/file_val.txt     \n",
      "  inflating: Corpus/file_train1.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip '/content/Corpus.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5i9JmGy9VoN"
   },
   "outputs": [],
   "source": [
    "with open('/content/Corpus/file_train.txt','w')as w:\n",
    "  with open('/content/Corpus/file_train1.txt','r')as r1:\n",
    "    with open('/content/Corpus/file_train2.txt','r')as r2:\n",
    "      w.write(r1.read())\n",
    "      w.write(r2.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_8ZO2jfCRXm9",
    "outputId": "4f14bfa2-83d0-4da3-d7b0-8a413e0e4646"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\t0\n",
      "<s>\t0\n",
      "</s>\t0\n",
      "[CLS]\t0\n",
      "[SEP]\t0\n",
      "[MASK]\t0\n",
      "▁।\t-3.07455\n",
      "ं\t-3.67803\n",
      "ः\t-3.80651\n",
      "।\t-3.99546\n",
      ",\t-4.08705\n",
      "▁\t-4.23016\n",
      "म्\t-4.50283\n",
      "-\t-4.57337\n",
      "े\t-4.62926\n",
      "▁इति\t-4.65191\n",
      "स्य\t-4.6726\n",
      "॒\t-4.69434\n",
      "▁च\t-4.82774\n",
      "॑\t-4.88225\n",
      "ा\t-4.99476\n",
      "▁न\t-5.20115\n",
      "ो\t-5.38825\n",
      "‍\t-5.43734\n",
      ":\t-5.51369\n",
      ".\t-5.51452\n",
      "ाः\t-5.57751\n",
      "▁एव\t-5.58343\n",
      "▁भवति\t-5.67858\n",
      "▁स\t-5.70426\n",
      "ेन\t-5.72247\n",
      "▁-\t-5.78285\n",
      "▁अ\t-5.81641\n",
      "ि\t-5.85707\n",
      "्\t-5.8782\n",
      "▁॥\t-5.91099\n",
      "▁अस्ति\t-5.92783\n",
      "य\t-5.96918\n",
      "म\t-5.97827\n",
      "▁“\t-5.97967\n",
      "▁अपि\t-6.08146\n",
      "ी\t-6.10417\n",
      "ानि\t-6.1287\n",
      "ति\t-6.14882\n",
      "▁वा\t-6.19074\n",
      "ात्\t-6.21225\n",
      "ेषु\t-6.21343\n",
      "न\t-6.21628\n",
      "▁\"\t-6.22172\n",
      "▁आसीत्\t-6.24364\n",
      "▁तस्य\t-6.24598\n",
      "त\t-6.29047\n",
      "ानां\t-6.2997\n",
      "व\t-6.31181\n",
      "ाय\t-6.33806\n",
      "ौ\t-6.47308\n",
      "या\t-6.4897\n",
      "\"\t-6.52072\n",
      "▁तु\t-6.53597\n",
      "▁यत्\t-6.593\n",
      "क\t-6.59759\n",
      "श्च\t-6.60327\n",
      "'\t-6.60739\n",
      "ान्\t-6.60875\n",
      "स्\t-6.61635\n",
      "त्\t-6.6187\n",
      "न्\t-6.6474\n",
      "▁सन्ति\t-6.65413\n",
      "▁तथा\t-6.65702\n",
      "▁सः\t-6.65874\n",
      "▁तत्र\t-6.67269\n",
      "र\t-6.69262\n",
      "▁हि\t-6.71228\n",
      "▁सह\t-6.72736\n",
      "▁प्र\t-6.73101\n",
      "ैः\t-6.74683\n",
      "▁आ\t-6.74751\n",
      "▁अस्य\t-6.76604\n",
      "▁अत्र\t-6.77732\n",
      "मिति\t-6.79169\n",
      "तः\t-6.79561\n",
      "▁—\t-6.81742\n",
      "ता\t-6.82082\n",
      "▁यथा\t-6.83471\n",
      "▁तेन\t-6.85962\n",
      "स\t-6.86171\n",
      "▁,\t-6.86803\n",
      "र्\t-6.88068\n",
      "्य\t-6.883\n",
      "’\t-6.92157\n",
      "”\t-6.93008\n",
      "▁तत्\t-6.96906\n",
      "▁अतः\t-6.9715\n",
      "▁‘\t-6.97562\n",
      "मेव\t-6.97825\n",
      "▁स्व\t-6.98691\n",
      "ऽ\t-6.99418\n",
      "▁स्म\t-6.99598\n",
      "▁'\t-7.01091\n",
      "▁ते\t-7.01663\n",
      "ते\t-7.01752\n",
      "▁प्रति\t-7.0373\n",
      "मपि\t-7.049\n",
      "ेण\t-7.06106\n",
      "▁य\t-7.06278\n",
      "श\t-7.06827\n",
      "पि\t-7.0836\n",
      "॥\t-7.09571\n",
      "▁अभवत्\t-7.10273\n",
      "▁स्यात्\t-7.10525\n",
      "▁सा\t-7.11393\n",
      "ना\t-7.13097\n",
      "▁नाम\t-7.13168\n",
      "प\t-7.14465\n",
      "▁अस्मिन्\t-7.15145\n",
      "ेति\t-7.17296\n",
      "▁तदा\t-7.17354\n",
      "▁वि\t-7.18666\n",
      "मा\t-7.20979\n",
      "ादि\t-7.2222\n",
      "अ\t-7.22432\n",
      "योः\t-7.22603\n",
      "तु\t-7.23942\n",
      "\t-7.24351\n",
      "▁–\t-7.24644\n",
      "द\t-7.2522\n",
      "वा\t-7.25329\n",
      "▁एवं\t-7.27994\n",
      "▁वर्तते\t-7.31627\n",
      "▁भवन्ति\t-7.32811\n",
      "▁ए\t-7.32936\n",
      "सु\t-7.33223\n",
      "▁;\t-7.35188\n",
      "त्वात्\t-7.37587\n",
      "तया\t-7.38837\n",
      "▁सर्व\t-7.39141\n",
      "▁?\t-7.40579\n",
      "▁मा\t-7.40746\n",
      "▁उ\t-7.42721\n",
      "नि\t-7.42942\n",
      "ण\t-7.43685\n",
      "त्वेन\t-7.43866\n",
      "▁वर्षे\t-7.44102\n",
      "ल\t-7.47028\n",
      "वि\t-7.47212\n",
      "च\t-7.4868\n",
      "▁किन्तु\t-7.49387\n",
      "▁१\t-7.49808\n",
      "▁वै\t-7.50283\n",
      "?\t-7.50356\n",
      "▁यदा\t-7.52265\n",
      "षु\t-7.52695\n",
      "▁सु\t-7.53352\n",
      "▁त\t-7.53877\n",
      "▁--\t-7.5454\n",
      "प्र\t-7.55954\n",
      "▁इदं\t-7.56067\n",
      "ु\t-7.5625\n",
      "▁एतत्\t-7.56542\n",
      "▁क\t-7.56553\n",
      "ह\t-7.57112\n",
      "▁तमे\t-7.57366\n",
      "▁इत्\n",
      "CPU times: user 2min 11s, sys: 708 ms, total: 2min 12s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Albert Tokenizer uses Sentence piece Tokenization, so I have used sentencepiece to to train tokenizer.\n",
    "#This will take a while\n",
    "spm.SentencePieceTrainer.Train('--input=/content/Corpus/file_train.txt \\\n",
    "                                --model_prefix=m \\\n",
    "                                --vocab_size=32000 \\\n",
    "                                --control_symbols=[CLS],[SEP],[MASK]')\n",
    "\n",
    "with open(\"m.vocab\") as v:\n",
    "    print(v.read(2000))\n",
    "    v.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OsnhjRiMBRBU"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W2jMAPtFRbsS",
    "outputId": "c69ac222-b78e-4a42-e556-98d78d053d43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data_dir/’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data_dir/\n",
    "!cp m.model -d /content/data_dir/spiece.model\n",
    "!cp m.vocab -d /content/data_dir/spiece.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUu9C3cBD3Rm"
   },
   "source": [
    "# Huggingface Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-LXoQM_FUS4"
   },
   "outputs": [],
   "source": [
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNAOMXjpMHZD",
    "outputId": "a1523114-8130-4e09-8f39-f96a78f2b31f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file /content/data_dir/added_tokens.json. We won't load it.\n",
      "Didn't find file /content/data_dir/special_tokens_map.json. We won't load it.\n",
      "Didn't find file /content/data_dir/tokenizer_config.json. We won't load it.\n",
      "loading file /content/data_dir/spiece.model\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "Adding <pad> to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#Keep in mind, This is a tokenizer for Albert, unlike the previous one, which is a generic one.\n",
    "#We'll load it in the form of Albert Tokenizer.\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"/content/data_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "fshtRIT71G49",
    "outputId": "e81dcf55-4641-41ec-acec-7085d942bf42"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'नैनं छिनदनति शसतराणि नैनं दहति पावकः। न चैनं कलेदयनतयापो न शोषयति मारुतः॥'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op = tokenizer.encode(\"नैनं छिन्दन्ति शस्त्राणि नैनं दहति पावकः। न चैनं क्लेदयन्त्यापो न शोषयति मारुतः॥\")\n",
    "tokenizer.decode(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rBDCTG4m73Td",
    "outputId": "a42689b4-8ef3-4ac0-ae4d-a8898a31db60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 11549, 7, 15152, 47, 3925, 43, 308, 85, 13898, 11549, 7, 21641, 23044, 8, 9, 21, 10569, 7, 2103, 14, 125, 1042, 134, 1668, 21, 7580, 165, 11604, 8, 107, 4]\n"
     ]
    }
   ],
   "source": [
    "print(op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vknw7FwU5oQ1"
   },
   "source": [
    "Looks like, the tokenizer is working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXX8IPifHDuk"
   },
   "source": [
    "## Model-Tokenizer Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cznfudYuHI9F"
   },
   "source": [
    "This is important.\n",
    "The training script needs a configuration for the model.\n",
    "\n",
    "Architecture refers to what the model is going to be used for\\\n",
    "i.e., AlbertModelForLM, or for Sequence Classification.\n",
    "Just take a look ar left panel for Model Architectures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XXbOamDnLL_b",
    "outputId": "17e3990f-2a1a-46ac-a46d-cf7a0b4823fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking vocabulary size\n",
    "vocab_size=tokenizer.vocab_size ; vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWEBZjCRKItM"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "config = {\n",
    "    \"architectures\": [\n",
    "        \"AlbertModel\"\n",
    "    ],\n",
    "\t\"attention_probs_dropout_prob\": 0.1,\n",
    "\t\"hidden_act\": \"gelu\",\n",
    "\t\"hidden_dropout_prob\": 0.1,\n",
    "\t\"hidden_size\": 768,\n",
    "\t\"initializer_range\": 0.02,\n",
    "\t\"intermediate_size\": 3072,\n",
    "\t\"layer_norm_eps\": 1e-05,\n",
    "\t\"max_position_embeddings\": 514,\n",
    "\t\"model_type\": \"albert\",\n",
    "\t\"num_attention_heads\": 12,\n",
    "\t\"num_hidden_layers\": 6,\n",
    "\t\"type_vocab_size\": 1,\n",
    "\t\"vocab_size\": vocab_size\n",
    "}\n",
    "with open(\"/content/data_dir/config.json\", 'w') as fp:\n",
    "    json.dump(config, fp)\n",
    "\n",
    "\n",
    "#Configuration for tokenizer.\n",
    "#Note: I set do_lower_case: False, and keep_accents:True\n",
    "\n",
    "tokenizer_config = {\n",
    "\t\"max_len\": 512,\n",
    "\t\"model_type\": \"albert\",\n",
    "\t\"do_lower_case\":False, \n",
    "\t\"keep_accents\":True\n",
    "}\n",
    "with open(\"/content/data_dir/tokenizer_config.json\", 'w') as fp:\n",
    "    json.dump(tokenizer_config, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KUdxTNaD9k6"
   },
   "source": [
    "**Note: **While experimenting with tokenizer training, I found that encoding was done corectly, but when decoding with {do_lower_case: True, and keep_accents:False}, the decoded sentence was a bit changed.\n",
    "\n",
    "So, by using above settings, I got the sentences decoded perfectly. \n",
    "a reason maybe that Sanskrit does not have 'Casing'. and the word has suffixes in the form of accents.\n",
    "\n",
    "You should try with the settings ehich suits best for your langugae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QfJp_WJq8BJg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uwPCraE3SzTY",
    "outputId": "5ad7e6c4-bed1-433a-a4e4-540720d76ca5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7v8tpoNE9-T"
   },
   "source": [
    "Checkpointing is very important. This is a directory where the intermediate model and tokenizer will be saved. \n",
    "\n",
    "**Note:** You should checkpoint to somewhere else, Maybe to your drive. and set \n",
    "`--save_total_limit 2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aHxYqnQFOLK"
   },
   "source": [
    "This is the training script. you should experiment with arguments.\n",
    " \n",
    "`!python /content/transformers/examples/run_language_modeling.py --help`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWuRBaqFx9vD"
   },
   "outputs": [],
   "source": [
    "!mkdir '/content/data_dir/mode'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ti5mbhXN2VDa",
    "outputId": "bb563894-d327-4487-8aec-7ee7060f6230"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |█                               | 10 kB 25.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 20 kB 23.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 30 kB 11.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 40 kB 8.8 MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 51 kB 4.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 61 kB 5.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 71 kB 5.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 81 kB 5.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 92 kB 6.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 102 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 112 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 122 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 133 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 143 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 153 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 163 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 174 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▏             | 184 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 194 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 204 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 215 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 225 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 235 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 245 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 256 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 266 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▏    | 276 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 286 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 296 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 307 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 317 kB 5.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 325 kB 5.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.4.0)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[K     |████████████████████████████████| 212 kB 45.8 MB/s \n",
      "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
      "\u001b[K     |████████████████████████████████| 136 kB 49.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 39.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 48.3 MB/s \n",
      "\u001b[?25hCollecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "\u001b[K     |████████████████████████████████| 144 kB 43.7 MB/s \n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 2.5 MB/s \n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
      "\u001b[K     |████████████████████████████████| 271 kB 49.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, datasets\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.24.3\n",
      "    Uninstalling urllib3-1.24.3:\n",
      "      Successfully uninstalled urllib3-1.24.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.0.0 frozenlist-1.3.0 fsspec-2022.3.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "urllib3"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_uaDWFF3ynr",
    "outputId": "11885903-5dfb-40cc-eb25-62ab92574e55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/03/2022 12:04:48 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "04/03/2022 12:04:48 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=20.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=/content/data_dir/mode,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/content/data_dir/mode,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=108,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "04/03/2022 12:04:48 - WARNING - datasets.builder - Using custom data configuration default-fdd981c00adfd308\n",
      "04/03/2022 12:04:48 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "04/03/2022 12:04:48 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-fdd981c00adfd308/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
      "04/03/2022 12:04:48 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-fdd981c00adfd308/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n",
      "04/03/2022 12:04:48 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-fdd981c00adfd308/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
      "100% 2/2 [00:00<00:00, 834.19it/s]\n",
      "[INFO|configuration_utils.py:651] 2022-04-03 12:04:48,696 >> loading configuration file /content/data_dir/config.json\n",
      "[INFO|configuration_utils.py:689] 2022-04-03 12:04:48,697 >> Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"/content/data_dir/\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1698] 2022-04-03 12:04:48,698 >> Didn't find file /content/data_dir/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1776] 2022-04-03 12:04:48,699 >> loading file /content/data_dir/spiece.model\n",
      "[INFO|tokenization_utils_base.py:1776] 2022-04-03 12:04:48,699 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1776] 2022-04-03 12:04:48,699 >> loading file /content/data_dir/added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1776] 2022-04-03 12:04:48,699 >> loading file /content/data_dir/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1776] 2022-04-03 12:04:48,699 >> loading file /content/data_dir/tokenizer_config.json\n",
      "[INFO|tokenization_utils.py:425] 2022-04-03 12:04:48,794 >> Adding <pad> to the vocabulary\n",
      "04/03/2022 12:04:48 - INFO - __main__ - Training new model from scratch\n",
      "Running tokenizer on dataset line_by_line:   0% 0/22 [00:00<?, ?ba/s]04/03/2022 12:04:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-fdd981c00adfd308/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-8b501b7a0c258108.arrow\n",
      "Running tokenizer on dataset line_by_line: 100% 22/22 [00:13<00:00,  1.59ba/s]\n",
      "Running tokenizer on dataset line_by_line:   0% 0/6 [00:00<?, ?ba/s]04/03/2022 12:05:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-fdd981c00adfd308/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-fc05a49c4f8d6675.arrow\n",
      "Running tokenizer on dataset line_by_line: 100% 6/6 [00:04<00:00,  1.40ba/s]\n",
      "[INFO|trainer.py:567] 2022-04-03 12:05:10,560 >> The following columns in the training set  don't have a corresponding argument in `AlbertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `AlbertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "[WARNING|training_args.py:999] 2022-04-03 12:05:10,562 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:999] 2022-04-03 12:05:10,562 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1290] 2022-04-03 12:05:10,571 >> ***** Running training *****\n",
      "[INFO|trainer.py:1291] 2022-04-03 12:05:10,571 >>   Num examples = 21964\n",
      "[INFO|trainer.py:1292] 2022-04-03 12:05:10,572 >>   Num Epochs = 20\n",
      "[INFO|trainer.py:1293] 2022-04-03 12:05:10,572 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1294] 2022-04-03 12:05:10,572 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1295] 2022-04-03 12:05:10,572 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1296] 2022-04-03 12:05:10,572 >>   Total optimization steps = 54920\n",
      "[WARNING|training_args.py:999] 2022-04-03 12:05:10,576 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:1013] 2022-04-03 12:05:10,576 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "{'loss': 9.3415, 'learning_rate': 4.954479242534596e-05, 'epoch': 0.18}\n",
      "  1% 500/54920 [04:26<6:52:57,  2.20it/s][INFO|trainer.py:2166] 2022-04-03 12:09:37,379 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 12:09:37,380 >> Configuration saved in /content/data_dir/mode/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 12:09:37,466 >> Model weights saved in /content/data_dir/mode/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 12:09:37,466 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 12:09:37,467 >> Special tokens file saved in /content/data_dir/mode/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 8.3876, 'learning_rate': 4.9089584850691914e-05, 'epoch': 0.36}\n",
      "  2% 1000/54920 [09:00<9:52:10,  1.52it/s][INFO|trainer.py:2166] 2022-04-03 12:14:10,861 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-1000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 12:14:10,862 >> Configuration saved in /content/data_dir/mode/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 12:14:10,949 >> Model weights saved in /content/data_dir/mode/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 12:14:10,950 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 12:14:10,950 >> Special tokens file saved in /content/data_dir/mode/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 8.1639, 'learning_rate': 4.863437727603787e-05, 'epoch': 0.55}\n",
      "  3% 1500/54920 [13:34<8:14:47,  1.80it/s][INFO|trainer.py:2166] 2022-04-03 12:18:44,920 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-1500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 12:18:44,921 >> Configuration saved in /content/data_dir/mode/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 12:18:45,004 >> Model weights saved in /content/data_dir/mode/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 12:18:45,005 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 12:18:45,005 >> Special tokens file saved in /content/data_dir/mode/checkpoint-1500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 12:18:45,338 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-500] due to args.save_total_limit\n",
      "{'loss': 8.0622, 'learning_rate': 4.817916970138383e-05, 'epoch': 0.73}\n",
      "  4% 2000/54920 [18:00<7:14:00,  2.03it/s][INFO|trainer.py:2166] 2022-04-03 12:23:11,306 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-2000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 12:23:11,307 >> Configuration saved in /content/data_dir/mode/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 12:23:11,383 >> Model weights saved in /content/data_dir/mode/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 12:23:11,383 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 12:23:11,383 >> Special tokens file saved in /content/data_dir/mode/checkpoint-2000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 12:23:11,661 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-1000] due to args.save_total_limit\n",
      "{'loss': 8.0374, 'learning_rate': 4.772396212672979e-05, 'epoch': 0.91}\n",
      "  5% 2500/54920 [22:20<5:23:14,  2.70it/s][INFO|trainer.py:2166] 2022-04-03 12:27:30,616 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-2500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 12:27:30,617 >> Configuration saved in /content/data_dir/mode/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 12:27:30,702 >> Model weights saved in /content/data_dir/mode/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 12:27:30,702 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 12:27:30,703 >> Special tokens file saved in /content/data_dir/mode/checkpoint-2500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 12:27:30,964 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-1500] due to args.save_total_limit\n",
      "{'loss': 7.9464, 'learning_rate': 4.726875455207575e-05, 'epoch': 1.09}\n",
      "  5% 3000/54920 [26:48<9:26:48,  1.53it/s][INFO|trainer.py:2166] 2022-04-03 12:31:58,665 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-3000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 12:31:58,667 >> Configuration saved in /content/data_dir/mode/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 12:31:58,748 >> Model weights saved in /content/data_dir/mode/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 12:31:58,749 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 12:31:58,749 >> Special tokens file saved in /content/data_dir/mode/checkpoint-3000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 12:31:59,007 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-2000] due to args.save_total_limit\n",
      "{'loss': 7.9189, 'learning_rate': 4.681354697742171e-05, 'epoch': 1.27}\n",
      "  6% 3500/54920 [31:13<8:33:05,  1.67it/s][INFO|trainer.py:2166] 2022-04-03 12:36:23,827 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-3500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 12:36:23,828 >> Configuration saved in /content/data_dir/mode/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 12:36:23,907 >> Model weights saved in /content/data_dir/mode/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 12:36:23,908 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 12:36:23,908 >> Special tokens file saved in /content/data_dir/mode/checkpoint-3500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 12:36:24,214 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-2500] due to args.save_total_limit\n",
      "{'loss': 7.8785, 'learning_rate': 4.635833940276766e-05, 'epoch': 1.46}\n",
      "  7% 4000/54920 [35:27<7:37:07,  1.86it/s][INFO|trainer.py:2166] 2022-04-03 12:40:38,289 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-4000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 12:40:38,290 >> Configuration saved in /content/data_dir/mode/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 12:40:38,370 >> Model weights saved in /content/data_dir/mode/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 12:40:38,371 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 12:40:38,371 >> Special tokens file saved in /content/data_dir/mode/checkpoint-4000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 12:40:38,707 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-3000] due to args.save_total_limit\n",
      "{'loss': 7.8355, 'learning_rate': 4.590313182811362e-05, 'epoch': 1.64}\n",
      "  8% 4500/54920 [39:54<8:54:08,  1.57it/s][INFO|trainer.py:2166] 2022-04-03 12:45:04,849 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-4500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 12:45:04,850 >> Configuration saved in /content/data_dir/mode/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 12:45:04,937 >> Model weights saved in /content/data_dir/mode/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 12:45:04,937 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 12:45:04,938 >> Special tokens file saved in /content/data_dir/mode/checkpoint-4500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 12:45:05,208 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-3500] due to args.save_total_limit\n",
      "{'loss': 7.7967, 'learning_rate': 4.544792425345958e-05, 'epoch': 1.82}\n",
      "  9% 5000/54920 [44:17<9:23:41,  1.48it/s][INFO|trainer.py:2166] 2022-04-03 12:49:28,088 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-5000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 12:49:28,089 >> Configuration saved in /content/data_dir/mode/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 12:49:28,170 >> Model weights saved in /content/data_dir/mode/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 12:49:28,171 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 12:49:28,171 >> Special tokens file saved in /content/data_dir/mode/checkpoint-5000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 12:49:28,447 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-4000] due to args.save_total_limit\n",
      "{'loss': 7.741, 'learning_rate': 4.4992716678805534e-05, 'epoch': 2.0}\n",
      " 10% 5500/54920 [48:43<5:42:39,  2.40it/s][INFO|trainer.py:2166] 2022-04-03 12:53:54,098 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-5500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 12:53:54,099 >> Configuration saved in /content/data_dir/mode/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 12:53:54,178 >> Model weights saved in /content/data_dir/mode/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 12:53:54,178 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 12:53:54,179 >> Special tokens file saved in /content/data_dir/mode/checkpoint-5500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 12:53:54,447 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-4500] due to args.save_total_limit\n",
      "{'loss': 7.7137, 'learning_rate': 4.453750910415149e-05, 'epoch': 2.18}\n",
      " 11% 6000/54920 [53:02<6:55:11,  1.96it/s][INFO|trainer.py:2166] 2022-04-03 12:58:12,953 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-6000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 12:58:12,954 >> Configuration saved in /content/data_dir/mode/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 12:58:13,037 >> Model weights saved in /content/data_dir/mode/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 12:58:13,038 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 12:58:13,038 >> Special tokens file saved in /content/data_dir/mode/checkpoint-6000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 12:58:13,312 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-5000] due to args.save_total_limit\n",
      "{'loss': 7.706, 'learning_rate': 4.408230152949745e-05, 'epoch': 2.37}\n",
      " 12% 6500/54920 [57:26<5:43:46,  2.35it/s][INFO|trainer.py:2166] 2022-04-03 13:02:36,990 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-6500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 13:02:36,991 >> Configuration saved in /content/data_dir/mode/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 13:02:37,071 >> Model weights saved in /content/data_dir/mode/checkpoint-6500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 13:02:37,072 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 13:02:37,072 >> Special tokens file saved in /content/data_dir/mode/checkpoint-6500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 13:02:37,366 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-5500] due to args.save_total_limit\n",
      "{'loss': 7.6565, 'learning_rate': 4.362709395484341e-05, 'epoch': 2.55}\n",
      " 13% 7000/54920 [1:01:54<6:34:29,  2.02it/s][INFO|trainer.py:2166] 2022-04-03 13:07:04,780 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-7000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 13:07:04,781 >> Configuration saved in /content/data_dir/mode/checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 13:07:04,863 >> Model weights saved in /content/data_dir/mode/checkpoint-7000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 13:07:04,863 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 13:07:04,864 >> Special tokens file saved in /content/data_dir/mode/checkpoint-7000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 13:07:05,175 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-6000] due to args.save_total_limit\n",
      "{'loss': 7.6481, 'learning_rate': 4.317188638018937e-05, 'epoch': 2.73}\n",
      " 14% 7500/54920 [1:06:22<9:05:56,  1.45it/s][INFO|trainer.py:2166] 2022-04-03 13:11:33,544 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-7500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 13:11:33,545 >> Configuration saved in /content/data_dir/mode/checkpoint-7500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 13:11:33,628 >> Model weights saved in /content/data_dir/mode/checkpoint-7500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 13:11:33,628 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-7500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 13:11:33,628 >> Special tokens file saved in /content/data_dir/mode/checkpoint-7500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 13:11:33,940 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-6500] due to args.save_total_limit\n",
      "{'loss': 7.6148, 'learning_rate': 4.271667880553532e-05, 'epoch': 2.91}\n",
      " 15% 8000/54920 [1:10:48<8:24:41,  1.55it/s][INFO|trainer.py:2166] 2022-04-03 13:15:59,405 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-8000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 13:15:59,406 >> Configuration saved in /content/data_dir/mode/checkpoint-8000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 13:15:59,490 >> Model weights saved in /content/data_dir/mode/checkpoint-8000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 13:15:59,491 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-8000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 13:15:59,491 >> Special tokens file saved in /content/data_dir/mode/checkpoint-8000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 13:15:59,832 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-7000] due to args.save_total_limit\n",
      "{'loss': 7.6177, 'learning_rate': 4.226147123088128e-05, 'epoch': 3.1}\n",
      " 15% 8500/54920 [1:15:15<5:27:00,  2.37it/s][INFO|trainer.py:2166] 2022-04-03 13:20:26,441 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-8500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 13:20:26,442 >> Configuration saved in /content/data_dir/mode/checkpoint-8500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 13:20:26,518 >> Model weights saved in /content/data_dir/mode/checkpoint-8500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 13:20:26,518 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-8500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 13:20:26,519 >> Special tokens file saved in /content/data_dir/mode/checkpoint-8500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 13:20:26,785 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-7500] due to args.save_total_limit\n",
      "{'loss': 7.5789, 'learning_rate': 4.180626365622724e-05, 'epoch': 3.28}\n",
      " 16% 9000/54920 [1:19:42<7:40:35,  1.66it/s][INFO|trainer.py:2166] 2022-04-03 13:24:52,927 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-9000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 13:24:52,928 >> Configuration saved in /content/data_dir/mode/checkpoint-9000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 13:24:53,011 >> Model weights saved in /content/data_dir/mode/checkpoint-9000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 13:24:53,011 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-9000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 13:24:53,011 >> Special tokens file saved in /content/data_dir/mode/checkpoint-9000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 13:24:53,339 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-8000] due to args.save_total_limit\n",
      "{'loss': 7.5596, 'learning_rate': 4.13510560815732e-05, 'epoch': 3.46}\n",
      " 17% 9500/54920 [1:24:13<7:48:39,  1.62it/s][INFO|trainer.py:2166] 2022-04-03 13:29:24,081 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-9500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 13:29:24,082 >> Configuration saved in /content/data_dir/mode/checkpoint-9500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 13:29:24,160 >> Model weights saved in /content/data_dir/mode/checkpoint-9500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 13:29:24,161 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 13:29:24,161 >> Special tokens file saved in /content/data_dir/mode/checkpoint-9500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 13:29:24,465 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-8500] due to args.save_total_limit\n",
      "{'loss': 7.5311, 'learning_rate': 4.089584850691915e-05, 'epoch': 3.64}\n",
      " 18% 10000/54920 [1:28:27<7:55:19,  1.58it/s][INFO|trainer.py:2166] 2022-04-03 13:33:37,956 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-10000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 13:33:37,957 >> Configuration saved in /content/data_dir/mode/checkpoint-10000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 13:33:38,038 >> Model weights saved in /content/data_dir/mode/checkpoint-10000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 13:33:38,038 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 13:33:38,039 >> Special tokens file saved in /content/data_dir/mode/checkpoint-10000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 13:33:38,363 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-9000] due to args.save_total_limit\n",
      "{'loss': 7.4982, 'learning_rate': 4.044064093226512e-05, 'epoch': 3.82}\n",
      " 19% 10500/54920 [1:33:01<5:41:00,  2.17it/s][INFO|trainer.py:2166] 2022-04-03 13:38:12,031 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-10500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 13:38:12,032 >> Configuration saved in /content/data_dir/mode/checkpoint-10500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 13:38:12,108 >> Model weights saved in /content/data_dir/mode/checkpoint-10500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 13:38:12,109 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-10500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 13:38:12,109 >> Special tokens file saved in /content/data_dir/mode/checkpoint-10500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 13:38:12,438 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-9500] due to args.save_total_limit\n",
      "{'loss': 7.5145, 'learning_rate': 3.998543335761107e-05, 'epoch': 4.01}\n",
      " 20% 11000/54920 [1:37:20<6:33:00,  1.86it/s][INFO|trainer.py:2166] 2022-04-03 13:42:31,371 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-11000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 13:42:31,372 >> Configuration saved in /content/data_dir/mode/checkpoint-11000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 13:42:31,449 >> Model weights saved in /content/data_dir/mode/checkpoint-11000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 13:42:31,449 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-11000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 13:42:31,450 >> Special tokens file saved in /content/data_dir/mode/checkpoint-11000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 13:42:31,782 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-10000] due to args.save_total_limit\n",
      "{'loss': 7.5019, 'learning_rate': 3.953022578295703e-05, 'epoch': 4.19}\n",
      " 21% 11500/54920 [1:41:40<7:19:31,  1.65it/s][INFO|trainer.py:2166] 2022-04-03 13:46:50,699 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-11500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 13:46:50,700 >> Configuration saved in /content/data_dir/mode/checkpoint-11500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 13:46:50,776 >> Model weights saved in /content/data_dir/mode/checkpoint-11500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 13:46:50,777 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-11500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 13:46:50,777 >> Special tokens file saved in /content/data_dir/mode/checkpoint-11500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 13:46:51,095 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-10500] due to args.save_total_limit\n",
      "{'loss': 7.4989, 'learning_rate': 3.907501820830299e-05, 'epoch': 4.37}\n",
      " 22% 12000/54920 [1:46:07<5:08:27,  2.32it/s][INFO|trainer.py:2166] 2022-04-03 13:51:18,017 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-12000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 13:51:18,018 >> Configuration saved in /content/data_dir/mode/checkpoint-12000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 13:51:18,094 >> Model weights saved in /content/data_dir/mode/checkpoint-12000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 13:51:18,095 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-12000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 13:51:18,095 >> Special tokens file saved in /content/data_dir/mode/checkpoint-12000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 13:51:18,415 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-11000] due to args.save_total_limit\n",
      "{'loss': 7.4342, 'learning_rate': 3.861981063364894e-05, 'epoch': 4.55}\n",
      " 23% 12500/54920 [1:50:33<4:34:14,  2.58it/s][INFO|trainer.py:2166] 2022-04-03 13:55:43,862 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-12500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 13:55:43,863 >> Configuration saved in /content/data_dir/mode/checkpoint-12500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 13:55:43,947 >> Model weights saved in /content/data_dir/mode/checkpoint-12500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 13:55:43,948 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-12500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 13:55:43,948 >> Special tokens file saved in /content/data_dir/mode/checkpoint-12500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 13:55:44,293 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-11500] due to args.save_total_limit\n",
      "{'loss': 7.4676, 'learning_rate': 3.81646030589949e-05, 'epoch': 4.73}\n",
      " 24% 13000/54920 [1:54:49<5:21:04,  2.18it/s][INFO|trainer.py:2166] 2022-04-03 14:00:00,122 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-13000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 14:00:00,123 >> Configuration saved in /content/data_dir/mode/checkpoint-13000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 14:00:00,208 >> Model weights saved in /content/data_dir/mode/checkpoint-13000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 14:00:00,209 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-13000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 14:00:00,209 >> Special tokens file saved in /content/data_dir/mode/checkpoint-13000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 14:00:00,526 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-12000] due to args.save_total_limit\n",
      "{'loss': 7.4659, 'learning_rate': 3.770939548434086e-05, 'epoch': 4.92}\n",
      " 25% 13500/54920 [1:59:19<5:48:10,  1.98it/s][INFO|trainer.py:2166] 2022-04-03 14:04:30,196 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-13500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 14:04:30,197 >> Configuration saved in /content/data_dir/mode/checkpoint-13500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 14:04:30,282 >> Model weights saved in /content/data_dir/mode/checkpoint-13500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 14:04:30,283 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-13500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 14:04:30,283 >> Special tokens file saved in /content/data_dir/mode/checkpoint-13500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 14:04:30,642 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-12500] due to args.save_total_limit\n",
      "{'loss': 7.4239, 'learning_rate': 3.725418790968681e-05, 'epoch': 5.1}\n",
      " 25% 14000/54920 [2:03:50<6:10:16,  1.84it/s][INFO|trainer.py:2166] 2022-04-03 14:09:00,588 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-14000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 14:09:00,589 >> Configuration saved in /content/data_dir/mode/checkpoint-14000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 14:09:00,673 >> Model weights saved in /content/data_dir/mode/checkpoint-14000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 14:09:00,673 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-14000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 14:09:00,674 >> Special tokens file saved in /content/data_dir/mode/checkpoint-14000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 14:09:01,022 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-13000] due to args.save_total_limit\n",
      "{'loss': 7.4182, 'learning_rate': 3.679898033503278e-05, 'epoch': 5.28}\n",
      " 26% 14500/54920 [2:08:14<6:14:30,  1.80it/s][INFO|trainer.py:2166] 2022-04-03 14:13:25,098 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-14500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 14:13:25,100 >> Configuration saved in /content/data_dir/mode/checkpoint-14500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 14:13:25,190 >> Model weights saved in /content/data_dir/mode/checkpoint-14500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 14:13:25,191 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-14500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 14:13:25,191 >> Special tokens file saved in /content/data_dir/mode/checkpoint-14500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 14:13:25,537 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-13500] due to args.save_total_limit\n",
      "{'loss': 7.38, 'learning_rate': 3.634377276037874e-05, 'epoch': 5.46}\n",
      " 27% 15000/54920 [2:12:39<5:06:12,  2.17it/s][INFO|trainer.py:2166] 2022-04-03 14:17:49,634 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-15000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 14:17:49,635 >> Configuration saved in /content/data_dir/mode/checkpoint-15000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 14:17:49,714 >> Model weights saved in /content/data_dir/mode/checkpoint-15000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 14:17:49,715 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-15000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 14:17:49,715 >> Special tokens file saved in /content/data_dir/mode/checkpoint-15000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 14:17:50,057 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-14000] due to args.save_total_limit\n",
      "{'loss': 7.3887, 'learning_rate': 3.588856518572469e-05, 'epoch': 5.64}\n",
      " 28% 15500/54920 [2:17:02<6:11:49,  1.77it/s][INFO|trainer.py:2166] 2022-04-03 14:22:12,940 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-15500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 14:22:12,942 >> Configuration saved in /content/data_dir/mode/checkpoint-15500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 14:22:13,023 >> Model weights saved in /content/data_dir/mode/checkpoint-15500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 14:22:13,024 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-15500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 14:22:13,024 >> Special tokens file saved in /content/data_dir/mode/checkpoint-15500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 14:22:13,355 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-14500] due to args.save_total_limit\n",
      "{'loss': 7.3636, 'learning_rate': 3.543335761107065e-05, 'epoch': 5.83}\n",
      " 29% 16000/54920 [2:21:31<8:09:20,  1.33it/s][INFO|trainer.py:2166] 2022-04-03 14:26:41,832 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-16000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 14:26:41,833 >> Configuration saved in /content/data_dir/mode/checkpoint-16000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 14:26:41,914 >> Model weights saved in /content/data_dir/mode/checkpoint-16000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 14:26:41,914 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-16000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 14:26:41,915 >> Special tokens file saved in /content/data_dir/mode/checkpoint-16000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 14:26:42,235 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-15000] due to args.save_total_limit\n",
      "{'loss': 7.3365, 'learning_rate': 3.497815003641661e-05, 'epoch': 6.01}\n",
      " 30% 16500/54920 [2:25:58<6:35:45,  1.62it/s][INFO|trainer.py:2166] 2022-04-03 14:31:09,194 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-16500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 14:31:09,195 >> Configuration saved in /content/data_dir/mode/checkpoint-16500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 14:31:09,282 >> Model weights saved in /content/data_dir/mode/checkpoint-16500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 14:31:09,283 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-16500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 14:31:09,283 >> Special tokens file saved in /content/data_dir/mode/checkpoint-16500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 14:31:09,639 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-15500] due to args.save_total_limit\n",
      "{'loss': 7.3593, 'learning_rate': 3.452294246176256e-05, 'epoch': 6.19}\n",
      " 31% 17000/54920 [2:30:17<4:54:56,  2.14it/s][INFO|trainer.py:2166] 2022-04-03 14:35:27,702 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-17000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 14:35:27,704 >> Configuration saved in /content/data_dir/mode/checkpoint-17000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 14:35:27,792 >> Model weights saved in /content/data_dir/mode/checkpoint-17000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 14:35:27,793 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-17000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 14:35:27,793 >> Special tokens file saved in /content/data_dir/mode/checkpoint-17000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 14:35:28,162 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-16000] due to args.save_total_limit\n",
      "{'loss': 7.3362, 'learning_rate': 3.406773488710852e-05, 'epoch': 6.37}\n",
      " 32% 17500/54920 [2:34:40<5:24:35,  1.92it/s][INFO|trainer.py:2166] 2022-04-03 14:39:50,863 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-17500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 14:39:50,865 >> Configuration saved in /content/data_dir/mode/checkpoint-17500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 14:39:50,953 >> Model weights saved in /content/data_dir/mode/checkpoint-17500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 14:39:50,954 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-17500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 14:39:50,954 >> Special tokens file saved in /content/data_dir/mode/checkpoint-17500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 14:39:51,309 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-16500] due to args.save_total_limit\n",
      "{'loss': 7.3269, 'learning_rate': 3.361252731245448e-05, 'epoch': 6.55}\n",
      " 33% 18000/54920 [2:39:09<6:35:52,  1.55it/s][INFO|trainer.py:2166] 2022-04-03 14:44:20,493 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-18000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 14:44:20,494 >> Configuration saved in /content/data_dir/mode/checkpoint-18000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 14:44:20,581 >> Model weights saved in /content/data_dir/mode/checkpoint-18000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 14:44:20,582 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-18000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 14:44:20,582 >> Special tokens file saved in /content/data_dir/mode/checkpoint-18000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 14:44:20,961 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-17000] due to args.save_total_limit\n",
      "{'loss': 7.2846, 'learning_rate': 3.315731973780044e-05, 'epoch': 6.74}\n",
      " 34% 18500/54920 [2:43:36<6:20:22,  1.60it/s][INFO|trainer.py:2166] 2022-04-03 14:48:47,435 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-18500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 14:48:47,436 >> Configuration saved in /content/data_dir/mode/checkpoint-18500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 14:48:47,529 >> Model weights saved in /content/data_dir/mode/checkpoint-18500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 14:48:47,530 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-18500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 14:48:47,530 >> Special tokens file saved in /content/data_dir/mode/checkpoint-18500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 14:48:47,827 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-17500] due to args.save_total_limit\n",
      "{'loss': 7.3066, 'learning_rate': 3.27021121631464e-05, 'epoch': 6.92}\n",
      " 35% 19000/54920 [2:48:07<3:43:31,  2.68it/s][INFO|trainer.py:2166] 2022-04-03 14:53:17,812 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-19000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 14:53:17,813 >> Configuration saved in /content/data_dir/mode/checkpoint-19000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 14:53:17,906 >> Model weights saved in /content/data_dir/mode/checkpoint-19000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 14:53:17,907 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-19000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 14:53:17,907 >> Special tokens file saved in /content/data_dir/mode/checkpoint-19000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 14:53:18,292 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-18000] due to args.save_total_limit\n",
      "{'loss': 7.3165, 'learning_rate': 3.224690458849236e-05, 'epoch': 7.1}\n",
      " 36% 19500/54920 [2:52:30<5:25:05,  1.82it/s][INFO|trainer.py:2166] 2022-04-03 14:57:41,470 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-19500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 14:57:41,471 >> Configuration saved in /content/data_dir/mode/checkpoint-19500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 14:57:41,559 >> Model weights saved in /content/data_dir/mode/checkpoint-19500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 14:57:41,560 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-19500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 14:57:41,560 >> Special tokens file saved in /content/data_dir/mode/checkpoint-19500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 14:57:41,917 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-18500] due to args.save_total_limit\n",
      "{'loss': 7.2761, 'learning_rate': 3.179169701383831e-05, 'epoch': 7.28}\n",
      " 36% 20000/54920 [2:56:57<6:49:48,  1.42it/s][INFO|trainer.py:2166] 2022-04-03 15:02:08,322 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-20000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 15:02:08,323 >> Configuration saved in /content/data_dir/mode/checkpoint-20000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 15:02:08,420 >> Model weights saved in /content/data_dir/mode/checkpoint-20000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 15:02:08,421 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-20000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 15:02:08,421 >> Special tokens file saved in /content/data_dir/mode/checkpoint-20000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 15:02:08,790 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-19000] due to args.save_total_limit\n",
      "{'loss': 7.2595, 'learning_rate': 3.133648943918427e-05, 'epoch': 7.47}\n",
      " 37% 20500/54920 [3:01:21<4:32:12,  2.11it/s][INFO|trainer.py:2166] 2022-04-03 15:06:31,832 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-20500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 15:06:31,833 >> Configuration saved in /content/data_dir/mode/checkpoint-20500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 15:06:31,923 >> Model weights saved in /content/data_dir/mode/checkpoint-20500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 15:06:31,924 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-20500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 15:06:31,924 >> Special tokens file saved in /content/data_dir/mode/checkpoint-20500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 15:06:32,295 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-19500] due to args.save_total_limit\n",
      "{'loss': 7.2557, 'learning_rate': 3.088128186453023e-05, 'epoch': 7.65}\n",
      " 38% 21000/54920 [3:05:50<3:00:31,  3.13it/s][INFO|trainer.py:2166] 2022-04-03 15:11:00,660 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-21000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 15:11:00,661 >> Configuration saved in /content/data_dir/mode/checkpoint-21000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 15:11:00,754 >> Model weights saved in /content/data_dir/mode/checkpoint-21000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 15:11:00,755 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-21000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 15:11:00,755 >> Special tokens file saved in /content/data_dir/mode/checkpoint-21000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 15:11:01,103 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-20000] due to args.save_total_limit\n",
      "{'loss': 7.2498, 'learning_rate': 3.0426074289876185e-05, 'epoch': 7.83}\n",
      " 39% 21500/54920 [3:10:15<4:25:25,  2.10it/s][INFO|trainer.py:2166] 2022-04-03 15:15:26,211 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-21500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 15:15:26,212 >> Configuration saved in /content/data_dir/mode/checkpoint-21500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 15:15:26,312 >> Model weights saved in /content/data_dir/mode/checkpoint-21500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 15:15:26,313 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-21500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 15:15:26,313 >> Special tokens file saved in /content/data_dir/mode/checkpoint-21500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 15:15:26,685 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-20500] due to args.save_total_limit\n",
      "{'loss': 7.2432, 'learning_rate': 2.997086671522214e-05, 'epoch': 8.01}\n",
      " 40% 22000/54920 [3:14:39<4:39:20,  1.96it/s][INFO|trainer.py:2166] 2022-04-03 15:19:50,318 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-22000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 15:19:50,319 >> Configuration saved in /content/data_dir/mode/checkpoint-22000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 15:19:50,409 >> Model weights saved in /content/data_dir/mode/checkpoint-22000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 15:19:50,410 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-22000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 15:19:50,410 >> Special tokens file saved in /content/data_dir/mode/checkpoint-22000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 15:19:50,801 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-21000] due to args.save_total_limit\n",
      "{'loss': 7.2392, 'learning_rate': 2.9515659140568103e-05, 'epoch': 8.19}\n",
      " 41% 22500/54920 [3:19:06<4:15:41,  2.11it/s][INFO|trainer.py:2166] 2022-04-03 15:24:17,497 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-22500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 15:24:17,498 >> Configuration saved in /content/data_dir/mode/checkpoint-22500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 15:24:17,589 >> Model weights saved in /content/data_dir/mode/checkpoint-22500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 15:24:17,589 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-22500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 15:24:17,589 >> Special tokens file saved in /content/data_dir/mode/checkpoint-22500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 15:24:17,978 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-21500] due to args.save_total_limit\n",
      "{'loss': 7.2443, 'learning_rate': 2.906045156591406e-05, 'epoch': 8.38}\n",
      " 42% 23000/54920 [3:23:33<4:10:38,  2.12it/s][INFO|trainer.py:2166] 2022-04-03 15:28:44,534 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-23000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 15:28:44,535 >> Configuration saved in /content/data_dir/mode/checkpoint-23000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 15:28:44,627 >> Model weights saved in /content/data_dir/mode/checkpoint-23000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 15:28:44,628 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-23000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 15:28:44,628 >> Special tokens file saved in /content/data_dir/mode/checkpoint-23000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 15:28:45,001 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-22000] due to args.save_total_limit\n",
      "{'loss': 7.2442, 'learning_rate': 2.8605243991260018e-05, 'epoch': 8.56}\n",
      " 43% 23500/54920 [3:28:03<3:42:39,  2.35it/s][INFO|trainer.py:2166] 2022-04-03 15:33:14,095 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-23500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 15:33:14,096 >> Configuration saved in /content/data_dir/mode/checkpoint-23500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 15:33:14,187 >> Model weights saved in /content/data_dir/mode/checkpoint-23500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 15:33:14,188 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-23500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 15:33:14,188 >> Special tokens file saved in /content/data_dir/mode/checkpoint-23500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 15:33:14,558 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-22500] due to args.save_total_limit\n",
      "{'loss': 7.2374, 'learning_rate': 2.8150036416605974e-05, 'epoch': 8.74}\n",
      " 44% 24000/54920 [3:32:30<4:21:46,  1.97it/s][INFO|trainer.py:2166] 2022-04-03 15:37:41,187 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-24000\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 15:37:41,188 >> Configuration saved in /content/data_dir/mode/checkpoint-24000/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 15:37:41,279 >> Model weights saved in /content/data_dir/mode/checkpoint-24000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 15:37:41,279 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-24000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 15:37:41,280 >> Special tokens file saved in /content/data_dir/mode/checkpoint-24000/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 15:37:41,634 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-23000] due to args.save_total_limit\n",
      "{'loss': 7.2227, 'learning_rate': 2.769482884195193e-05, 'epoch': 8.92}\n",
      " 45% 24500/54920 [3:37:04<5:49:38,  1.45it/s][INFO|trainer.py:2166] 2022-04-03 15:42:15,215 >> Saving model checkpoint to /content/data_dir/mode/checkpoint-24500\n",
      "[INFO|configuration_utils.py:440] 2022-04-03 15:42:15,216 >> Configuration saved in /content/data_dir/mode/checkpoint-24500/config.json\n",
      "[INFO|modeling_utils.py:1377] 2022-04-03 15:42:15,305 >> Model weights saved in /content/data_dir/mode/checkpoint-24500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-03 15:42:15,306 >> tokenizer config file saved in /content/data_dir/mode/checkpoint-24500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-03 15:42:15,306 >> Special tokens file saved in /content/data_dir/mode/checkpoint-24500/special_tokens_map.json\n",
      "[INFO|trainer.py:2244] 2022-04-03 15:42:15,676 >> Deleting older checkpoint [/content/data_dir/mode/checkpoint-23500] due to args.save_total_limit\n",
      " 45% 24793/54920 [3:39:37<3:37:03,  2.31it/s]"
     ]
    }
   ],
   "source": [
    "#To train from scratch\n",
    "!python /content/transformers/examples/pytorch/language-modeling/run_mlm.py \\\n",
    "        --model_type albert-base-v2 \\\n",
    "        --config_name /content/data_dir/ \\\n",
    "        --tokenizer_name /content/data_dir/ \\\n",
    "        --train_file /content/Corpus/file_train.txt \\\n",
    "        --validation_file /content/Corpus/file_val.txt \\\n",
    "        --output_dir /content/data_dir/mode \\\n",
    "        --do_train \\\n",
    "        --do_eval \\\n",
    "        --mlm_probability 0.15\\\n",
    "        --line_by_line \\\n",
    "        --save_steps 500 \\\n",
    "        --logging_steps 500 \\\n",
    "        --save_total_limit 2 \\\n",
    "        --num_train_epochs 20 \\\n",
    "        --per_gpu_eval_batch_size 8 \\\n",
    "        --per_gpu_train_batch_size 8 \\\n",
    "        --seed 108 \\\n",
    "        --logging_dir logs \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EoVLzMqbRNml"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rto4PJn0RRho"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtZXHQycUlSL"
   },
   "outputs": [],
   "source": [
    "from transformers import AlbertForMaskedLM,AlbertConfig,AlbertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0gWpM1RU1WW"
   },
   "outputs": [],
   "source": [
    "sans_tokenizer = AlbertTokenizer.from_pretrained(\"/content/data_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xZD6w3hVfhX"
   },
   "outputs": [],
   "source": [
    "sans_tokenizer.save_pretrained(\"/content/data_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ETixiIlSH9Q"
   },
   "outputs": [],
   "source": [
    "!zip -r \"/content/data_dir.zip\" \"/content/data_dir\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_Kz6AL8SGQU"
   },
   "outputs": [],
   "source": [
    "!cp -r '/content/data_dir.zip' '/content/gdrive/My Drive/sans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ysj56aMd_juw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wz-I_cW6kL4f"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BF2Zm7lLBsKo"
   },
   "outputs": [],
   "source": [
    "!mkdir sanskrit_albert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DivzVIZGDDon"
   },
   "outputs": [],
   "source": [
    "atokenizer = AlbertTokenizer.from_pretrained(\"/content/data_dir\")\n",
    "atokenizer.save_pretrained(\"/content/sanskrit_albert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJFATzaZTqlC"
   },
   "outputs": [],
   "source": [
    "!zip -r \"/content/sanskrit_albert.zip\" \"/content/sanskrit_albert\" \n",
    "!cp -r '/content/sanskrit_albert.zip' '/content/gdrive/My Drive/sans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WkHmzUaIDMNC",
    "outputId": "267e906e-d787-4e61-a364-79d5f103f5f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ॐ असतो मा सद्गमय । तमसो मा ज्योतिर्गमय । मृत्योर्मा अमृतं गमय । ॐ शान्तिः शान्तिः शान्तिः ॥\n"
     ]
    }
   ],
   "source": [
    "op = atokenizer.encode(\"ॐ असतो मा सद्गमय । तमसो मा ज्योतिर्गमय । मृत्योर्मा अमृतं गमय । ॐ शान्तिः शान्तिः शान्तिः ॥\")\n",
    "print(atokenizer.decode(op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-GqI2RQDNjq",
    "outputId": "28b60452-f3f4-40ec-ede0-d23d8d1185fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1900, 6, 25476, 146, 29, 31946, 6, 7, 6, 15924, 146, 6608, 9954, 6, 7, 3232, 8571, 1426, 8, 6, 9954, 6, 7, 1900, 1145, 9, 1145, 9, 1145, 9, 86, 4]\n"
     ]
    }
   ],
   "source": [
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K34oH9UhDRY4",
    "outputId": "18f099e7-17e0-42b9-b173-c4540629f867"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /content/data_dir/mode/checkpoint-14500/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"/content/data_dir/\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 32001\n",
      "}\n",
      "\n",
      "loading weights file /content/data_dir/mode/checkpoint-14500/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /content/data_dir/mode/checkpoint-14500 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertModel were not initialized from the model checkpoint at /content/data_dir/mode/checkpoint-14500 and are newly initialized: ['albert.pooler.weight', 'albert.pooler.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Configuration saved in /content/sanskrit_albert/config.json\n",
      "Model weights saved in /content/sanskrit_albert/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model = AlbertModel.from_pretrained(\"/content/data_dir/mode/checkpoint-14500\")\n",
    "model.save_pretrained(\"/content/sanskrit_albert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QlPBuXIqEfxx",
    "outputId": "7eb3191d-5590-4791-d0b5-74a13a0ccd0d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file sanskrit_albert/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"/content/data_dir/mode/checkpoint-14500\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 32001\n",
      "}\n",
      "\n",
      "loading weights file sanskrit_albert/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing AlbertModel.\n",
      "\n",
      "All the weights of AlbertModel were initialized from the model checkpoint at sanskrit_albert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "modle = AlbertModel.from_pretrained(\"sanskrit_albert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "na3KI3qREq04",
    "outputId": "25a68022-378c-4a94-db4d-d436e23d1e43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertModel(\n",
       "  (embeddings): AlbertEmbeddings(\n",
       "    (word_embeddings): Embedding(32001, 128, padding_idx=0)\n",
       "    (position_embeddings): Embedding(514, 128)\n",
       "    (token_type_embeddings): Embedding(1, 128)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): AlbertTransformer(\n",
       "    (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "    (albert_layer_groups): ModuleList(\n",
       "      (0): AlbertLayerGroup(\n",
       "        (albert_layers): ModuleList(\n",
       "          (0): AlbertLayer(\n",
       "            (full_layer_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): AlbertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (pooler_activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modle.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbtyp_THEtiu",
    "outputId": "d083de23-019e-41de-be09-6c46fbaf56a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "अपि सवरणमयी लङका न मे लकषमण रोचते । जननी जनमभूमिशच सवरगादपि गरीयसी ॥\n"
     ]
    }
   ],
   "source": [
    "enc=tokenizer.encode(\"अपि स्वर्णमयी लङ्का न मे लक्ष्मण रोचते । जननी जन्मभूमिश्च स्वर्गादपि गरीयसी ॥\")\n",
    "enc2=tokenizer.encode(\" द्वौ अतीव प्रसन्नौ अभवताम् ।\")\n",
    "print(sans_tokenizer.decode(enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "quUpqMPdEzjR",
    "outputId": "fed78d3d-d5ae-47cc-f365-5ef3c05a0849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 49, 7645, 1464, 14414, 19728, 318, 19, 224, 974, 11546, 10627, 4124, 6, 7, 18102, 379, 34, 10893, 119, 148, 29, 951, 241, 3677, 6, 3275, 13252, 86, 4]\n",
      "[3, 6, 11328, 45, 776, 18951, 43, 5788, 9286, 836, 6, 7, 4]\n"
     ]
    }
   ],
   "source": [
    "print(enc)\n",
    "print(enc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UeFLnCuhE2D5"
   },
   "outputs": [],
   "source": [
    "ps = modle(torch.tensor(enc).unsqueeze(1))\n",
    "ps2 = modle(torch.tensor(enc2).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xww-_uRoE4PD",
    "outputId": "6b1b3c6f-130a-49d4-8c35-4c5045d4bff3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 1.5935, -0.3140,  1.3185,  ...,  1.0577, -1.3886, -0.6393]],\n",
      "\n",
      "        [[ 0.1111,  0.3287,  1.6674,  ...,  0.7312, -2.0979,  2.0125]],\n",
      "\n",
      "        [[ 1.5501,  0.8265,  0.9667,  ...,  0.1805, -0.2189,  0.8105]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.4571,  0.5136,  0.9735,  ...,  0.8732,  0.5766, -0.6602]],\n",
      "\n",
      "        [[-0.6474, -0.2640,  1.7862,  ...,  0.0913, -1.8094,  1.8944]],\n",
      "\n",
      "        [[ 1.9137, -0.6117,  0.5470,  ...,  1.1358, -1.0411, -0.4403]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.3679, -0.2001, -0.6820,  ...,  0.1531, -0.1977,  0.6285],\n",
      "        [-0.1048,  0.4288, -0.4792,  ..., -0.3077,  0.0980,  0.4149],\n",
      "        [-0.2848,  0.2737, -0.5344,  ..., -0.3420, -0.1532,  0.1855],\n",
      "        ...,\n",
      "        [-0.2401,  0.0402, -0.3002,  ...,  0.2079, -0.2072,  0.5684],\n",
      "        [ 0.2128,  0.3796, -0.6251,  ...,  0.1136,  0.3396,  0.4983],\n",
      "        [-0.5040, -0.2417, -0.5659,  ...,  0.3729,  0.0229,  0.7925]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, attentions=None)\n",
      "BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 1.5935, -0.3140,  1.3185,  ...,  1.0577, -1.3886, -0.6393]],\n",
      "\n",
      "        [[ 0.9236,  0.4739,  0.4156,  ...,  1.0536, -1.1088,  1.2331]],\n",
      "\n",
      "        [[ 2.5088,  0.2349,  0.6997,  ...,  1.0471, -0.1965, -0.6707]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.9236,  0.4739,  0.4156,  ...,  1.0536, -1.1088,  1.2331]],\n",
      "\n",
      "        [[ 1.6228,  0.8925, -0.5417,  ...,  0.0565,  0.6081,  0.6840]],\n",
      "\n",
      "        [[ 1.9137, -0.6117,  0.5470,  ...,  1.1358, -1.0411, -0.4403]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.3679, -0.2001, -0.6820,  ...,  0.1531, -0.1977,  0.6285],\n",
      "        [-0.4467,  0.0153,  0.4623,  ...,  0.5536,  0.4333,  0.6718],\n",
      "        [-0.5725, -0.1663, -0.5503,  ...,  0.0810, -0.2389,  0.6692],\n",
      "        ...,\n",
      "        [-0.4467,  0.0153,  0.4623,  ...,  0.5536,  0.4333,  0.6718],\n",
      "        [-0.4669, -0.7263, -0.2562,  ...,  0.5964,  0.1540,  0.7594],\n",
      "        [-0.5040, -0.2417, -0.5659,  ...,  0.3729,  0.0229,  0.7925]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(ps)\n",
    "print(ps2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Albert_mlm_trainer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
