{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):|\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-04-25T09:44:17.361522Z","iopub.execute_input":"2023-04-25T09:44:17.362425Z","iopub.status.idle":"2023-04-25T09:44:17.367697Z","shell.execute_reply.started":"2023-04-25T09:44:17.362373Z","shell.execute_reply":"2023-04-25T09:44:17.366441Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install transformers GPUtil numba rouge bertviz","metadata":{"execution":{"iopub.status.busy":"2023-04-25T09:44:17.404866Z","iopub.execute_input":"2023-04-25T09:44:17.405819Z","iopub.status.idle":"2023-04-25T09:44:36.777093Z","shell.execute_reply.started":"2023-04-25T09:44:17.405776Z","shell.execute_reply":"2023-04-25T09:44:36.775399Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.27.4)\nCollecting GPUtil\n  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numba in /opt/conda/lib/python3.7/site-packages (0.56.4)\nCollecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\nCollecting bertviz\n  Downloading bertviz-1.4.0-py3-none-any.whl (157 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.6/157.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /opt/conda/lib/python3.7/site-packages (from numba) (0.39.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba) (59.8.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from rouge) (1.16.0)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from bertviz) (1.26.100)\nRequirement already satisfied: torch>=1.0 in /opt/conda/lib/python3.7/site-packages (from bertviz) (1.13.0+cpu)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from bertviz) (0.1.97)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3->bertviz) (0.6.0)\nCollecting botocore<1.30.0,>=1.29.100\n  Downloading botocore-1.29.119-py3-none-any.whl (10.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->bertviz) (1.0.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.100->boto3->bertviz) (2.8.2)\nBuilding wheels for collected packages: GPUtil\n  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7409 sha256=f1cf35c82199813a4755e1857d7a8b1e9d76f034ca55ac244c78e8ef40ec514a\n  Stored in directory: /root/.cache/pip/wheels/b1/e7/99/2b32600270cf23194c9860f029d3d5db075f250bc39028c045\nSuccessfully built GPUtil\nInstalling collected packages: GPUtil, rouge, botocore, bertviz\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.27.59\n    Uninstalling botocore-1.27.59:\n      Successfully uninstalled botocore-1.27.59\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.4.2 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.29.119 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed GPUtil-1.4.0 bertviz-1.4.0 botocore-1.29.119 rouge-1.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-04-25T09:44:36.779896Z","iopub.execute_input":"2023-04-25T09:44:36.780338Z","iopub.status.idle":"2023-04-25T09:44:36.786545Z","shell.execute_reply.started":"2023-04-25T09:44:36.780291Z","shell.execute_reply":"2023-04-25T09:44:36.785407Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import re\nimport os\nimport csv\nimport copy\nimport math\nimport time\nimport datetime\nimport tqdm\nimport logging\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport torch\nimport glob\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Linear, Dropout\nimport inspect\nfrom collections import OrderedDict\nfrom torch.nn import LayerNorm\nfrom torch.nn.init import xavier_uniform_\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda as cuda1\nfrom torch.utils.data.distributed import DistributedSampler, Sampler\nfrom torch.nn.modules.container import ModuleList\nfrom torch.optim.lr_scheduler import *\nfrom torch.optim import *\nfrom bertviz import model_view, head_view\nfrom rouge import Rouge\nfrom pandas.errors import *\nfrom torch.nn.modules.transformer import _get_activation_fn, _get_clones\nfrom torch.utils.data import TensorDataset, DataLoader, SequentialSampler, RandomSampler\nfrom torch.nn.modules import TransformerDecoderLayer as ImportedTransformerDecoderLayer\nfrom transformers import *\nfrom torch.optim.lr_scheduler import *","metadata":{"execution":{"iopub.status.busy":"2023-04-25T09:44:36.787798Z","iopub.execute_input":"2023-04-25T09:44:36.788132Z","iopub.status.idle":"2023-04-25T09:44:53.481204Z","shell.execute_reply.started":"2023-04-25T09:44:36.788099Z","shell.execute_reply":"2023-04-25T09:44:53.480042Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# data_dir = '/home/msc2/Project/Sanskrit_Data'\n# save_dir = '/home/msc2/Project/Sanskrit_Data/Models/'\n# model_path = '/media/msc2/A6982E8F982E5DD5/SEM3/Project/Models/bert-trained/'\n\ndata_dir = f'/kaggle/input/forbert/'\nsave_dir = f'/kaggle/working/'\nmodel_path = '/kaggle/input/forbert/san-bert/bert-trained/'\n\ndevice  = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2023-04-25T09:44:53.483820Z","iopub.execute_input":"2023-04-25T09:44:53.484777Z","iopub.status.idle":"2023-04-25T09:44:53.493899Z","shell.execute_reply.started":"2023-04-25T09:44:53.484734Z","shell.execute_reply":"2023-04-25T09:44:53.492423Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"model = BertModel.from_pretrained(model_path, output_attentions = True)\ntokenizer = BertTokenizer.from_pretrained(model_path)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T09:44:53.495905Z","iopub.execute_input":"2023-04-25T09:44:53.496219Z","iopub.status.idle":"2023-04-25T09:44:59.594219Z","shell.execute_reply.started":"2023-04-25T09:44:53.496186Z","shell.execute_reply":"2023-04-25T09:44:59.592802Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"loading configuration file /kaggle/input/forbert/san-bert/bert-trained/config.json\nModel config BertConfig {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"cls_token\": \"[CLS]\",\n  \"do_lower_case\": true,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"mask_token\": \"[MASK]\",\n  \"max_len\": 512,\n  \"max_position_embeddings\": 512,\n  \"model_max_length\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_attentions\": true,\n  \"pad_token\": \"[PAD]\",\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"sep_token\": \"[SEP]\",\n  \"transformers_version\": \"4.27.4\",\n  \"type_vocab_size\": 2,\n  \"unk_token\": \"[UNK]\",\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file /kaggle/input/forbert/san-bert/bert-trained/pytorch_model.bin\nSome weights of the model checkpoint at /kaggle/input/forbert/san-bert/bert-trained/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertModel were not initialized from the model checkpoint at /kaggle/input/forbert/san-bert/bert-trained/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nloading file vocab.txt\nloading file added_tokens.json\nloading file special_tokens_map.json\nloading file tokenizer_config.json\nloading configuration file /kaggle/input/forbert/san-bert/bert-trained/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"/kaggle/input/forbert/san-bert/bert-trained/\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"cls_token\": \"[CLS]\",\n  \"do_lower_case\": true,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"mask_token\": \"[MASK]\",\n  \"max_len\": 512,\n  \"max_position_embeddings\": 512,\n  \"model_max_length\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token\": \"[PAD]\",\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"sep_token\": \"[SEP]\",\n  \"transformers_version\": \"4.27.4\",\n  \"type_vocab_size\": 2,\n  \"unk_token\": \"[UNK]\",\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"def fix_state_dict(checkpoint):\n    new_state_dict = OrderedDict()\n    for k, v in checkpoint.items():\n        name = k.replace(\"module.\", \"\")\n        new_state_dict[name] = v\n    return new_state_dict\n#model.load_state_dict(fix_state_dict(checkpoint), strict=False);","metadata":{"execution":{"iopub.status.busy":"2023-04-25T09:44:59.595970Z","iopub.execute_input":"2023-04-25T09:44:59.596657Z","iopub.status.idle":"2023-04-25T09:44:59.602197Z","shell.execute_reply.started":"2023-04-25T09:44:59.596613Z","shell.execute_reply":"2023-04-25T09:44:59.601149Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def gpu_mem(string = ''):\n    if torch.cuda.is_available():\n        print(string)\n        gpu_usage()\n\ndef get_cuda_status():\n    if torch.cuda.is_available():\n        print('Device count: ', torch.cuda.device_count())\n        print('Current device: ',device)\n\ndef free_gpu_cache(stats = False):\n    if torch.cuda.is_available():\n        if stats:    \n            print('\\nInitial GPU usage')\n            gpu_usage()\n        torch.cuda.empty_cache()\n        cuda1.select_device(0)\n        cuda1.close()\n        cuda1.select_device(0)\n        if stats:\n            print('\\nGPU usage after emptying the cache')\n            gpu_usage()\n\ndef _get_subsequent_mask(src_sz, tgt_sz):\n    mask = (torch.triu(torch.ones(src_sz, tgt_sz)) == 1).transpose(0,1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask==1, float(0.0))\n    return mask\n\ndef _get_square_subsequent_mask(sz):\n    return _get_subsequent_mask(sz, sz)\n\ndef Embed(num_embeddings, embedding_dim, padding_idx):\n    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx = padding_idx)\n    nn.init.normal_(m.weight, mean = 0, std = embedding_dim**(-.5))\n    nn.init.constant_(m.weight[padding_idx], 0)\n    return m\n\ndef padding_trg(trg_ids, trg_ground_ids, trg_key_padding_mask, max_len, pad_token):\n    '''This function helps in the padding of the target data\n    Returns : target_ids, target_real_values (for calculating loss), target_mask.'''\n    trg_len = len(trg_ids)\n    trg_mask = [1]*trg_len\n    trg_padding = [pad_token]*(max_len - trg_len)\n    trg_mask_padding = [pad_token]*(max_len - trg_len)\n    trg_ids += trg_padding\n    trg_ground_ids+= trg_padding\n    trg_mask+= trg_mask_padding\n    return trg_ids, trg_ground_ids, trg_mask\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, embedding_dim, dropout = 0.1, max_seq_len = 512):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p = dropout)\n        pe = torch.zeros(max_seq_len, embedding_dim)\n        position = torch.arange(0, max_seq_len, dtype = torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n        pe[:, 0::2] = torch.sin(position*div_term)\n        pe[:, 1::2] = torch.cos(position*div_term)\n        pe = pe.unsqueeze(0).transpose(0,1)\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n    \ndef delete_data_tensors():\n    os.remove(data_dir+'/train.pt')\n    os.remove(data_dir+'/dev.pt')\n    os.remove(data_dir+'/test.pt')\n    \ndef clear_dir(dirname):\n    for f in glob.glob(dirname+'*'):\n        os.remove(f)\n    print('Success!')\n\ndef print_model_status(model):\n    print(next(model.parameters()).is_cuda)\n\ndef get_clones(module, layers):\n    module_list = [copy.deepcopy(module).to(device) for i in range(layers)]\n    #gpu_mem()\n    return ModuleList(module_list)\n\ndef retrieve_name(var):\n    for fi in reversed(inspect.stack()):\n        names = [var_name for var_name, var_val in fi.frame.f_locals.items() if var_val is var]\n        if len(names) > 0:\n            return names[0]\n\ndef printf(var, shape = True):\n    if shape == True:\n        print(f'Size of {retrieve_name(var)} : {var.shape}')\n        return\n    print(f'\\n{retrieve_name(var)} : {var}\\n----------------------------------------------------------------------------------------------------------------')\n    return\n\ndef decode_text(var, back = False):\n    if back == True:\n        return tokenizer.decode(var)\n    print(f'\\n{retrieve_name(var)} : {tokenizer.decode(var)}\\n----------------------------------------------------------------------------------------------------------------')\n    return\n\ndef decode(var, back = False):\n    outs = []\n    if back == True:\n        for i in range(var.shape[0]):\n            outs.append(decode_text(var[i], back = True))\n        return outs\n    for i in range(var.shape[0]):\n        decode_text(var[i])\n    return\n\ndef write_one(data, file_path):\n    with open(file_path, 'a+') as f:\n        f.write(data)\n        f.write('\\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n')\n    f.close()\n    return\n\ndef write_to_file(data, file_path):\n    outs = decode(data, back = True)\n    for out in outs:\n        write_one(out, file_path)\n    return\n\nfree_gpu_cache(stats = True)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T09:44:59.603904Z","iopub.execute_input":"2023-04-25T09:44:59.604347Z","iopub.status.idle":"2023-04-25T09:44:59.633371Z","shell.execute_reply.started":"2023-04-25T09:44:59.604307Z","shell.execute_reply":"2023-04-25T09:44:59.632309Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# clear_dir(save_dir)\n#os.remove('/kaggle/working/predictions.txt')","metadata":{"execution":{"iopub.status.busy":"2023-04-25T09:44:59.634729Z","iopub.execute_input":"2023-04-25T09:44:59.635723Z","iopub.status.idle":"2023-04-25T09:44:59.646260Z","shell.execute_reply.started":"2023-04-25T09:44:59.635683Z","shell.execute_reply":"2023-04-25T09:44:59.645231Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, decoder_layer, decoder_final_layer, num_layers, norm = None):\n        super(Decoder, self).__init__()\n        decoder_layer = decoder_layer\n        self.layers = get_clones(decoder_layer , num_layers-1)\n        self.final_layer = decoder_final_layer\n        self.num_layers = num_layers\n        self.norm = norm\n        \n    def forward(self, tgt, mem, tgt_mask = None, mem_mask = None, tgtkey_pad_mask = None, memkey_pad_mask = None):\n        output = tgt\n        for i in range(self.num_layers-1):\n            output = self.layers[i](output, mem,\n                                    tgt_mask = tgt_mask, memory_mask = mem_mask,\n                                    tgt_key_padding_mask = tgtkey_pad_mask,\n                                    memory_key_padding_mask = memkey_pad_mask,\n                                    \n                                   )\n        # output and mem of the shapes [Seq, Batch_size, Embedding_dim]\n        output, attn_weights = self.final_layer(output, mem,\n                                                tgt_mask = tgt_mask, mem_mask = mem_mask,\n                                                tgtkey_pad_mask = tgtkey_pad_mask,\n                                                memkey_pad_mask = memkey_pad_mask\n                                                )\n        if self.norm:\n            output = self.norm(output)\n        return output, attn_weights","metadata":{"execution":{"iopub.status.busy":"2023-04-25T09:44:59.647674Z","iopub.execute_input":"2023-04-25T09:44:59.648702Z","iopub.status.idle":"2023-04-25T09:44:59.657968Z","shell.execute_reply.started":"2023-04-25T09:44:59.648663Z","shell.execute_reply":"2023-04-25T09:44:59.656730Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class TransformerDecoderLayer(nn.Module):\n    def __init__(self, d_model = 768, nheads = 12, dim_ffn = 3072, dropout = 0.1):\n        super(TransformerDecoderLayer, self).__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nheads, dropout=dropout, device = device)\n        self.multihead_attn = nn.MultiheadAttention(d_model, nheads, dropout=dropout, device = device)\n        self.fc1 = Linear(d_model, dim_ffn)\n        self.dropout = Dropout(dropout)\n        self.fc2 = Linear(dim_ffn, d_model)\n        self.norm1 = LayerNorm(d_model)\n        self.norm2 = LayerNorm(d_model)\n        self.norm3 = LayerNorm(d_model)\n        self.dropout1 = Dropout(dropout)\n        self.dropout2 = Dropout(dropout)\n        self.dropout3 = Dropout(dropout)\n    \n    def forward(self, tgt, memory, tgt_mask = None, memory_mask = None, tgt_key_padding_mask = None, memory_key_padding_mask = None):\n        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask = tgt_mask, key_padding_mask = tgt_key_padding_mask)[0]\n        tgt = tgt + self.dropout1(tgt2)\n        tgt = self.norm1(tgt)\n        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask = memory_mask, key_padding_mask = memory_key_padding_mask)[0]\n        tgt = tgt + self.dropout2(tgt2)\n        tgt = self.norm2(tgt)\n        tgt2 = self.fc2(self.dropout(nn.LeakyReLU(0.1)(self.fc1(tgt))))\n        tgt = tgt + self.dropout3(tgt2)\n        tgt = self.norm3(tgt)\n        return tgt","metadata":{"execution":{"iopub.status.busy":"2023-04-25T09:44:59.663712Z","iopub.execute_input":"2023-04-25T09:44:59.664219Z","iopub.status.idle":"2023-04-25T09:44:59.675954Z","shell.execute_reply.started":"2023-04-25T09:44:59.664183Z","shell.execute_reply":"2023-04-25T09:44:59.674856Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class FinalDecoderLayer(nn.Module):\n    def __init__(self, d_model = 768, nheads = 12, dim_ffn = 3072, dropout = 0.1):\n        super(FinalDecoderLayer, self).__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nheads, dropout = dropout, device = device)\n        self.dropout1 = nn.Dropout(p = dropout)\n        self.norm1 = LayerNorm(d_model)\n        self.multihead_attn = nn.MultiheadAttention(d_model, nheads, dropout = dropout, device = device)\n        self.dropout2 = Dropout(p = dropout)\n        self.norm2 = LayerNorm(d_model)\n        self.fc1 = Linear(d_model, dim_ffn)\n        self.dropout = Dropout(p = dropout)\n        self.fc2 = Linear(dim_ffn, d_model)    \n        self.dropout3 = Dropout(p = dropout)\n        self.norm3 = LayerNorm(d_model)\n        \n    def forward(self, tgt, mem, tgt_mask = None, mem_mask = None, tgtkey_pad_mask = None,memkey_pad_mask = None):\n        tgt2 = self.self_attn(tgt, tgt, tgt,\n                              attn_mask=tgt_mask,\n                              key_padding_mask=tgtkey_pad_mask)[0]\n        tgt = tgt + self.dropout1(tgt2)\n        tgt = self.norm1(tgt)\n        tgt2, attn_dist = self.multihead_attn(tgt, mem, mem,\n                                              attn_mask=mem_mask,\n                                              key_padding_mask=memkey_pad_mask, \n                                              need_weights = True, average_attn_weights = False)\n        tgt = tgt + self.dropout2(tgt2)\n        tgt = self.norm2(tgt)\n        tgt2 = self.fc2(self.dropout(nn.LeakyReLU(0.1)(self.fc1(tgt))))\n        tgt = tgt + self.dropout3(tgt2)\n        tgt = self.norm3(tgt)\n        return tgt, attn_dist","metadata":{"execution":{"iopub.status.busy":"2023-04-25T09:44:59.677411Z","iopub.execute_input":"2023-04-25T09:44:59.678092Z","iopub.status.idle":"2023-04-25T09:44:59.692036Z","shell.execute_reply.started":"2023-04-25T09:44:59.678043Z","shell.execute_reply":"2023-04-25T09:44:59.690933Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class Seq2seq(nn.Module):\n    def __init__(self, src_vocab_size = 128, tgt_vocab_size = 128,\n                 embedding_dim = 768, fcn_hidden_dim = 3072,\n                 num_heads = 12, num_layers = 12, dropout = 0.1, src_tgt_mat= None):\n        super(Seq2seq, self).__init__()\n        self.src_vocab_size = src_vocab_size\n        self.tgt_vocab_size = tgt_vocab_size\n        self.embedding_dim = embedding_dim\n        self.src2tgt = src_tgt_mat\n        self.encoder = model.to(device)\n        self.src_embed = copy.deepcopy(self.encoder.embeddings)\n        self.tgt_embed = copy.deepcopy(self.encoder.embeddings)\n        self.decoder_layer = ImportedTransformerDecoderLayer(embedding_dim, num_heads, fcn_hidden_dim, dropout, activation = 'relu').to(device)\n        self.decoder_final_layer = FinalDecoderLayer(embedding_dim, num_heads, fcn_hidden_dim, dropout).to(device)\n        self.decoder = Decoder(self.decoder_layer, self.decoder_final_layer, num_layers).to(device)\n        self.p_vocab = nn.Sequential(nn.Linear(self.embedding_dim*2, fcn_hidden_dim),\n                                     nn.Linear(fcn_hidden_dim, self.tgt_vocab_size),\n                                     nn.Softmax(dim = -1))\n        self.src_mask = None\n        self.mem_mask = None\n        self.tgt_mask = None\n\n        \n    def return_boolean_masks(self, src_input_mask, tgt_input_mask):\n        srckey_pad_mask = (src_input_mask == 0)\n        tgtkey_pad_mask = (tgt_input_mask == 0)\n        memkey_pad_mask = copy.deepcopy(srckey_pad_mask).to(device)\n        return srckey_pad_mask, tgtkey_pad_mask, memkey_pad_mask\n    \n    def encode(self, src, srckey_pad_mask = None):\n        encoder_out = self.encoder(src, attention_mask = srckey_pad_mask)\n        mem = encoder_out['last_hidden_state'].transpose(0,1)\n        return mem\n\n    def decode(self, mem, src, tgt, memkey_pad_mask = None, tgtkey_pad_mask = None, has_mask = True):\n        self.tgt_mask = _get_square_subsequent_mask(tgt.size(1)).to(device) # [T,T]\n        tgt_embed = self.tgt_embed(tgt).transpose(0,1)\n        decoder_out, attn = self.decoder(tgt_embed, mem, tgt_mask = self.tgt_mask, mem_mask = self.mem_mask,\n                                         memkey_pad_mask = memkey_pad_mask, tgtkey_pad_mask = tgtkey_pad_mask\n                                        ) # returns attention of only the last layer.\n        # decoder_out of the shape [T,N,E]\n        # attn of the shape [N,num_heads,T,S]\n        A_t = attn.sum(dim = 1)\n        context_vecs = torch.matmul(A_t, mem.transpose(0,1)).transpose(0,1)\n        p_vocab = self.p_vocab(torch.cat((decoder_out, context_vecs), dim = -1)).transpose(0,1)\n        return p_vocab, A_t, context_vecs, decoder_out, tgt_embed\n        #return p_vocab\n        \n    def forward(self, src, tgt, src_input_mask = None, tgt_input_mask = None, has_mask = True):\n        srckey_pad_mask, tgtkey_pad_mask, memkey_pad_mask = self.return_boolean_masks(src_input_mask, tgt_input_mask)\n        mem = self.encode(src, src_input_mask)\n        out = self.decode(mem, src, tgt, memkey_pad_mask, tgtkey_pad_mask)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-04-25T09:44:59.693349Z","iopub.execute_input":"2023-04-25T09:44:59.693691Z","iopub.status.idle":"2023-04-25T09:44:59.710185Z","shell.execute_reply.started":"2023-04-25T09:44:59.693659Z","shell.execute_reply":"2023-04-25T09:44:59.709107Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class PGN(nn.Module):\n    def __init__(self, src_vocab_size = 128, tgt_vocab_size = 128,\n                 embedding_dim = 768, fcn_hidden_dim = 3072,\n                 num_heads = 12, num_layers = 12, dropout = 0.1, src_tgt_mat= None):\n        super(PGN, self).__init__()\n        self.src_vocab_size = src_vocab_size\n        self.baseline = Seq2seq(src_vocab_size = src_vocab_size,\n                             tgt_vocab_size = tgt_vocab_size,\n                             num_layers = num_layers,\n                             src_tgt_mat = src_tgt_mat,\n                             embedding_dim = embedding_dim\n                            ).to(device)\n        self.p_gen = nn.Sequential(nn.Linear(embedding_dim*3,1),\n                                   nn.Sigmoid()\n                                  )\n        self.conversion_matrix = src_tgt_mat\n          \n    def forward(self, src, tgt, src_input_mask = None, tgt_input_mask = None, has_mask = True):\n        srckey_pad_mask, tgtkey_pad_mask, memkey_pad_mask = self.baseline.return_boolean_masks(src_input_mask, tgt_input_mask)\n        mem = self.baseline.encode(src, src_input_mask)\n        p_vocab, A_t, context_vecs, decoder_out, tgt_embed = self.baseline.decode(mem, src, tgt, memkey_pad_mask, tgtkey_pad_mask)\n        total_states = torch.cat((context_vecs, decoder_out, tgt_embed), dim = -1)\n        p_gen = self.p_gen(total_states)\n        p_copy = 1 - p_gen\n        oh = torch.zeros(src.size(0), src.size(1), self.src_vocab_size, device = device)\n        oh_scattered = oh.scatter_(dim = -1, index = src.unsqueeze(-1), value = 1)\n        p_copy_src_vocab = torch.matmul(A_t, oh)\n        p_copy_tgt_vocab = torch.matmul(p_copy_src_vocab, self.conversion_matrix)\n        p = torch.add(p_gen * p_vocab.transpose(0,1), p_copy * p_copy_tgt_vocab.transpose(0,1))\n        return p.transpose(0,1)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T09:44:59.711758Z","iopub.execute_input":"2023-04-25T09:44:59.712139Z","iopub.status.idle":"2023-04-25T09:44:59.724995Z","shell.execute_reply.started":"2023-04-25T09:44:59.712105Z","shell.execute_reply":"2023-04-25T09:44:59.724075Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"vocab = tokenizer.get_vocab()\ninv_vocab = {v:k for k,v in vocab.items()}\n\nfor key, value in tokenizer.special_tokens_map.items():\n    print(f'{key} : {value} : {vocab[value]}')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-04-25T09:44:59.726224Z","iopub.execute_input":"2023-04-25T09:44:59.727048Z","iopub.status.idle":"2023-04-25T09:44:59.750898Z","shell.execute_reply.started":"2023-04-25T09:44:59.727010Z","shell.execute_reply":"2023-04-25T09:44:59.749694Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"unk_token : [UNK] : 1\nsep_token : [SEP] : 3\npad_token : [PAD] : 0\ncls_token : [CLS] : 2\nmask_token : [MASK] : 4\n","output_type":"stream"}]},{"cell_type":"code","source":"class ASansRoBERTa(nn.Module):\n    def __init__(self, args):\n        super(ASansRoBERTa, self).__init__()\n        self.save_params = args\n        self.max_len = args['max_len']\n        self.world_size = args['gpus']\n        self.train_batch_size = args['train_batch_size']\n        self.eval_batch_size = args['eval_batch_size']\n        self.epochs = args['epochs']\n        self.label_smooth = args['label_smooth']\n        self.tokenizer = tokenizer\n        self.vocab = self.tokenizer.get_vocab()\n        self.inv_vocab = {v:k for k,v in vocab.items()}\n        self.vocab_size = len(self.vocab)\n        self.unk_id = self.vocab['[UNK]']\n        self.sep_id = self.vocab['[SEP]']\n        self.pad_id = self.vocab['[PAD]']\n        self.cls_id = self.vocab['[CLS]']\n        self.max_src_len = args['src_seq_len']\n        self.max_tgt_len = args['tgt_seq_len']\n        self.num_layers = args['num_layers']\n        self.optimizer_lr = args['optimizer_lr']\n        self.optimizer_eps = args['optimizer_eps']\n        self.optimizer_wd = args['optimizer_wd']\n        self.scheduler_patience = args['scheduler_patience']\n        self.embedding_dim = args['embedding_dim']\n        self.num_heads = args['num_heads']\n        self.dropout = args['dropout']\n        self.scorer = Rouge()\n        self.scheduler = None\n        self.loss_fn = None\n        self.train_data = self.load_train_data(args['train_file'], data_dir+'train.pt', is_test = False)\n        self.dev_data = self.load_train_data(args['dev_file'], data_dir+'dev.pt', is_test = False)\n        self.test_data = self.load_test_data(args['test_file'], data_dir+'test.pt')\n        self.conversion_matrix = self.conversion_matrix_()\n        \n#         self.model = Seq2seq(src_vocab_size = self.vocab_size,\n#                              tgt_vocab_size = self.vocab_size,\n#                              num_layers = self.num_layers,\n#                              src_tgt_mat = self.conversion_matrix,\n#                              embedding_dim = self.embedding_dim,\n#                              num_heads = self.num_heads,\n#                              dropout = self.dropout,\n#                              fcn_hidden_dim = args['fcn_hidden_dim']\n#                             ).to(device)\n        \n        self.model = PGN(src_vocab_size = self.vocab_size,\n                             tgt_vocab_size = self.vocab_size,\n                             num_layers = self.num_layers,\n                             src_tgt_mat = self.conversion_matrix,\n                             embedding_dim = self.embedding_dim,\n                             num_heads = self.num_heads,\n                             dropout = self.dropout,\n                             fcn_hidden_dim = args['fcn_hidden_dim']\n                            ).to(device)\n        print_model_status(model)\n        \n    def conversion_matrix_(self):\n        conversion_matrix = torch.zeros(self.vocab_size, self.vocab_size)\n        src_vocab_items = self.vocab.items()\n        for src_token, src_id in src_vocab_items:\n            tgt_id = self.vocab.get(src_token, self.unk_id)\n            conversion_matrix[src_id][tgt_id] = 1\n        return conversion_matrix.to(device)\n    \n    def get_params(self, param_name = None):\n        self.save_params['train_data'] = self.train_data\n        self.save_params['test_data'] = self.test_data\n        self.save_params['val_data'] = self.dev_data\n        self.save_params['model'] = self.model\n        self.save_params['conv_mat'] = self.conversion_matrix\n        if param_name is not None:\n            return self.save_params[param_name]\n        return self.save_params\n    \n    def load_train_data(self, file_path, loader_path, is_test = False):\n        '''\n            This function is for loading and saving the training data.\n            From the training data we get the source ids and mask and the target ids, mask and ground truth values\n            This saves the input data in a pt object and this is further used for the training.\n        '''\n        if os.path.exists(loader_path):\n            print('Loading pre embedded data!')\n            data = torch.load(loader_path)\n        else:\n            print('Creating Embeddings!')\n            try:\n                f = pd.read_csv(file_path)\n            except:\n                f = pd.read_csv(file_path, sep = \"\\t\")\n            display(f)\n            src_data = f['text'].tolist()\n            if not is_test:\n                tgt_data = f['summary'].tolist()\n            #print(src_data[:5])\n            encoded_dict = self.tokenizer.batch_encode_plus(src_data,\n                                                       add_special_tokens = True,\n                                                       max_length = self.max_src_len,\n                                                       padding = 'max_length',\n                                                       return_attention_mask = True,\n                                                       truncation = True,\n                                                       return_tensors = 'pt')\n            src_input_ids = encoded_dict['input_ids']\n            src_attention_masks = encoded_dict['attention_mask']\n            if not is_test:\n#                 target_dict = self.tokenizer.batch_encode_plus(src_data,\n#                                                               add_special_tokens = True,\n#                                                               max_length = self.max_tgt_len,\n#                                                               padding = 'max_length',\n#                                                               return_attention_mask = True,\n#                                                               truncation = True,\n#                                                               return_tensors = 'pt')\n                \n                tgt_input_ids, tgt_ground_ids, tgt_attention_masks = [], [], []\n                for text in tgt_data:\n                    encoded_text = self.tokenizer(text)\n                    tgt_ids, tgt_attention_mask = encoded_text['input_ids'], encoded_text['attention_mask']\n                    if len(tgt_ids) > self.max_tgt_len:\n                        print('len larger than maxlen')\n                        tgt_ids = tgt_ids[:self.max_tgt_len-1]+[self.sep_id]\n                        tgt_attention_mask = tgt_attention_mask[:self.max_tgt_len]\n                    #print(tgt_ids)\n                    #print(tgt_attention_mask)\n                    tgt_input, tgt_ground, tgt_mask = padding_trg(tgt_ids[:-1],\n                                                                  tgt_ids[1:],\n                                                                  tgt_attention_mask[:-1],\n                                                                  self.max_tgt_len, self.pad_id)\n                    tgt_input_ids.append(tgt_input)\n                    tgt_ground_ids.append(tgt_ground)\n                    tgt_attention_masks.append(tgt_mask)\n                data = {\n                    'src_input_ids' : src_input_ids,\n                    'src_attention_masks': src_attention_masks,\n                    'trg_input_ids': torch.tensor(tgt_input_ids),\n                    'trg_ground_ids': torch.tensor(tgt_ground_ids),\n                    'trg_attention_masks': torch.tensor(tgt_attention_masks)\n                }\n            else:\n                data = {\n                    'src_input_ids': src_input_ids,\n                    'src_attention_masks': src_attention_masks\n                }\n            torch.save(data, loader_path)\n        return data\n    \n    def load_test_data(self, file_path, loader_path):\n        '''Loads the test data and saves it as a tensor object for further usage.'''\n        if os.path.exists(loader_path):\n            data = torch.load(loader_path)\n        else:\n            src_data = []\n            \n            f = pd.read_csv(file_path)\n            src_data = f['text'].tolist()\n            encoded_dict = tokenizer.batch_encode_plus(src_data,\n                                                       add_special_tokens=True,\n                                                       max_length=self.max_src_len,\n                                                       padding='max_length',\n                                                       return_attention_mask=True,\n                                                       truncation=True,\n                                                       return_tensors='pt')\n            src_input_ids = encoded_dict['input_ids']\n            src_attention_masks = encoded_dict['attention_mask']\n            data = {\n                'src_input_ids': src_input_ids,\n                'src_attention_masks': src_attention_masks\n            }\n            torch.save(data, loader_path)\n        return data\n\n    def my_dataloader(self, data_dict, batch_size, shuffle = True):\n        '''Creates dataloaders for training and testing data.'''\n        if 'trg_input_ids' in data_dict:\n            dataset = TensorDataset(data_dict['src_input_ids'],\n                                    data_dict['src_attention_masks'],\n                                    data_dict[\"trg_input_ids\"],\n                                    data_dict[\"trg_ground_ids\"],\n                                    data_dict[\"trg_attention_masks\"])\n        else:\n            dataset = TensorDataset(data_dict[\"src_input_ids\"],\n                                    data_dict[\"src_attention_masks\"])\n\n        if shuffle:\n            sampler = RandomSampler(dataset)\n            dataset_loader = DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n        else:\n            dataset_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n        return dataset_loader\n    \n    \n    def get_loss(self, predict, target, label_smooth = 0.1):\n        predict = predict.contiguous().view(-1, self.vocab_size)\n        target = target.contiguous().view(-1, 1)\n        non_pad_mask = target.ne(self.pad_id)\n        nll_loss = -predict.gather(dim = -1, index = target)[non_pad_mask].mean()\n        smooth_loss = -predict.sum(dim = -1, keepdim = True)[non_pad_mask].mean()\n        smooth_loss = smooth_loss/self.vocab_size\n        loss = ((1. - label_smooth) * nll_loss) + (label_smooth * smooth_loss)\n        return loss\n    \n    def write_data_to_file(self, data):\n        preds = data.topk(1)[1].squeeze()\n        write_to_file(preds, '/kaggle/working/predictions.txt')\n    \n    def get_accuracy(self, predict, target, mode, batch_size = 2, mask = None):\n        preds = predict.topk(1)[1].squeeze()\n        score = 0\n        score_prev = 0\n        if mode in ['test', 'train', 'valid']:\n            predicted = decode(preds, back = True)\n            ground_truths = decode(target, back = True)\n            for i in range(len(ground_truths)):\n                score_prev = self.scorer.get_scores(predicted[i], ground_truths[i], avg = True)\n                score += score_prev['rouge-1']['f']\n            return score\n        if mode in ['train', 'valid']:\n            accuracy = 0\n            if mask == None or batch_size == None:\n                print('Not enough information!')\n                return 0.0\n            preds = preds * mask\n            for i in range(batch_size):\n                non_zero_mask = preds[i].ne(0.0)\n                total_value = non_zero_mask.sum().item()\n                correct_pred = torch.eq(preds[i][non_zero_mask], target[i][non_zero_mask]).sum().item()\n                accuracy += correct_pred * (100.0 / total_value)\n            return accuracy/batch_size\n    \n    def validation(self, epoch):\n        dev_loader = self.my_dataloader(self.dev_data, self.eval_batch_size)\n        self.model.eval()\n        running_loss = 0\n        accuracy = 0\n        i = 0\n        with torch.no_grad():\n            for batch in dev_loader:\n                src_input_ids, src_input_mask = batch[0].to(device), batch[1].to(device)\n                tgt_input_ids, tgt_input_mask = batch[2].to(device), batch[4].to(device)\n                tgt_ground_ids = batch[3].to(device)\n                out = self.model(src_input_ids, tgt_input_ids, src_input_mask, tgt_input_mask)\n                loss = self.loss_fn(torch.log(out.transpose(1,2)), tgt_ground_ids)\n                #loss = self.get_loss(out, tgt_ground_ids)\n                running_loss += loss.item()\n                accuracy += self.get_accuracy(out, tgt_ground_ids, 'valid', self.eval_batch_size, mask = tgt_input_mask)\n                self.write_data_to_file(out)\n                #self.scheduler.step(loss)\n                del src_input_ids\n                del src_input_mask\n                del tgt_input_ids\n                del tgt_input_mask\n                del tgt_ground_ids\n                i += 1\n        final_loss = running_loss / (i+1)\n        accuracy = accuracy / len(dev_loader)\n        print(f\"Validation Epoch : {epoch}, Average Validation Loss : {final_loss:.4f}, Validation Accuracy : {accuracy:.2f}%\")\n        #print(f\"Validation Epoch : {epoch}, Average Validation Loss : {final_loss:.4f}\")\n        return final_loss\n    \n    def train(self, pre_model_path = None, pre_checkpoint_path = None, baseline_checkpoint_path = None, flag = False):\n        if pre_model_path:\n            print('Restoring Model from checkpoint!')\n            model = torch.load(pre_model_path+'pgn_model.pt')\n            checkpoint = torch.load(pre_model_path+'pgn_model_checkpoint.pt')\n            self.model = model\n            \n        if pre_checkpoint_path:\n            print('Restoring Model from checkpoint!')\n            checkpoint = torch.load(pre_checkpoint_path+'pgn_model_checkpoint.pt')\n            self.model.load_state_dict(fix_state_dict(checkpoint['model_state_dict']), strict = False)\n        \n        if baseline_checkpoint_path:\n            print('Restoring Baseline model only')\n            baseline_checkpoint = torch.load(baseline_checkpoint_path+'model_checkpoint.pt')\n            self.model.baseline.load_state_dict(fix_state_dict(baseline_checkpoint['model_state_dict']), strict = False)\n        \n        train_loader = self.my_dataloader(self.train_data, self.train_batch_size)\n        start_epoch = 0\n        epochs = self.epochs\n        if pre_model_path or pre_checkpoint_path:\n            print('Restoring Epochs!')\n            start_epoch = checkpoint['epoch']+1\n            epochs = epochs + start_epoch\n\n#         used to freeze encoder params and train for pgn\n        optimizer_grouped_params = [\n            #{'params': self.model.baseline.encoder.parameters(), 'lr': 1e-5, 'weight_decay': self.optimizer_wd}, # roberta encoder\n            #{'params': self.model.baseline.tgt_embed.parameters(), 'lr': 1e-5, 'weight_decay': 1e-3}, # model embeddings\n            {'params': self.model.baseline.decoder.parameters(), 'lr': self.optimizer_lr}, # roberta decoder\n            {'params': self.model.baseline.p_vocab.parameters(), 'lr': self.optimizer_lr}, # p_vocab\n            {'params': self.model.p_gen.parameters(), 'lr': self.optimizer_lr}] # p_gen\n\n#         optimizer_grouped_params = [\n#             {'params': self.model.encoder.parameters(), 'lr': 1e-5, 'weight_decay': self.optimizer_wd}, # roberta encoder\n#             {'params': self.model.tgt_embed.parameters(), 'lr': 1e-5, 'weight_decay': 1e-3}, # model embeddings\n#             {'params': self.model.decoder.parameters(), 'lr': self.optimizer_lr}, # roberta decoder\n#             {'params': self.model.p_vocab.parameters(), 'lr': self.optimizer_lr}] # p_vocab\n    \n        loss_fn = nn.NLLLoss(reduction = 'mean', ignore_index = self.pad_id) # ignore_index = self.pad_id\n        #loss_fn = nn.CrossEntropyLoss(reduction = 'mean', ignore_index = self.pad_id, label_smoothing = 0.1)\n        self.loss_fn = loss_fn\n        \n        optimizer = AdamW(optimizer_grouped_params, eps = self.optimizer_eps)\n        total_steps = len(train_loader) * epochs\n        \n        #scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = total_steps/10, num_training_steps = total_steps)\n        #scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = total_steps/10, num_training_steps = total_steps)\n        #scheduler = ReduceLROnPlateau(optimizer, mode = 'min', patience = 3, verbose = True)\n        #scheduler = CosineAnnealingWarmRestarts(optimizer, T_0 = 5, T_mult = 1,verbose = True)\n        scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps = total_steps/10, num_training_steps = total_steps)\n        self.scheduler = scheduler\n        \n        if pre_model_path or pre_checkpoint_path:\n            print('Optimizer and Scheduler state_dict restored!')\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            if flag == True:\n                self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n            else:\n                print('Learning Rate and Scheduler Changed!')\n\n        is_best = False\n        curr_valid_loss = 0\n        plot_train_loss = []\n        plot_valid_loss = []\n        if pre_checkpoint_path or pre_model_path:\n            plot_train_loss = checkpoint['plot_train_loss']\n            plot_valid_loss = checkpoint['plot_valid_loss']\n        best_valid_loss = float('inf')\n        epochs_no_improve = 0\n        total_steps = 0\n        stime = time.ctime()\n        start_time = time.monotonic()\n        print(f'Training started at : {stime}')\n        for epoch in range(start_epoch, epochs):\n            self.model.train()\n            print(f'Epoch / Total Epochs : {epoch} / {epochs}')\n            running_loss = 0.0\n            accuracy = 0.0\n            for batch in tqdm(train_loader):\n                src_input_ids, src_input_mask = batch[0].to(device), batch[1].to(device)\n                tgt_input_ids, tgt_input_mask = batch[2].to(device), batch[4].to(device)\n                tgt_ground_ids = batch[3].to(device)\n                optimizer.zero_grad()\n                out = self.model(src_input_ids, tgt_input_ids, src_input_mask, tgt_input_mask) # out of shape (batch_size, max_len, tgt_vocab_dist)\n                del src_input_ids\n                del src_input_mask\n                del tgt_input_ids\n                loss = self.loss_fn(torch.log(out.transpose(1,2)), tgt_ground_ids)\n                #loss = self.get_loss(out, tgt_ground_ids)\n                accuracy +=  self.get_accuracy(out, tgt_ground_ids, 'train', self.train_batch_size, tgt_input_mask)\n                self.write_data_to_file(out)\n                del tgt_input_mask\n                del tgt_ground_ids\n                loss.backward()\n                optimizer.step()\n                #self.scheduler.step(loss)                \n                running_loss += loss.item()\n                total_steps += 1\n            plot_train_loss.append(loss.item())\n            if total_steps % 100 == 0 or epoch % 1 == 0:\n                print(f'Train Epoch : {epoch}, Average Training Loss : {running_loss/len(train_loader):.4f}, Train Accuracy : {accuracy/len(train_loader):.2f}%')\n                #print(f'Train Epoch : {epoch}, Average Training Loss : {running_loss/len(train_loader):.4f}')\n            if epoch % 1 == 0:\n                epochs_no_improve += 1\n                curr_valid_loss = self.validation(epoch)\n                self.scheduler.step(curr_valid_loss)\n                plot_valid_loss.append(curr_valid_loss)\n                if curr_valid_loss < best_valid_loss:\n                    print('New Best Loss!')\n                    is_best = True\n                    best_valid_loss = curr_valid_loss\n                    best_valid_epoch = epoch\n                    epochs_no_improve = 0\n                    #torch.save(self.model, save_dir+f'model.pt')\n                    torch.save({\n                        'train_loss' : running_loss,\n                        'valid_loss' : best_valid_loss,\n                        'plot_train_loss' : plot_train_loss,\n                        'plot_valid_loss' : plot_valid_loss,\n                        'epoch' : epoch,\n                        'optimizer_state_dict' : optimizer.state_dict(),\n                        'scheduler_state_dict' : self.scheduler.state_dict(),\n                        'model_state_dict' : self.model.state_dict()}, save_dir+f'pgn_model_checkpoint1.pt')\n                if epochs_no_improve > 5:\n                    print('No change in validation loss. Training not useful! Stopping Training.')\n                    break\n        etime = time.ctime()\n        end_time = time.monotonic()\n        print(f'Training ends at : {etime}. Took almost {(end_time - start_time)/60} mins.')","metadata":{"execution":{"iopub.status.busy":"2023-04-25T09:44:59.753078Z","iopub.execute_input":"2023-04-25T09:44:59.753951Z","iopub.status.idle":"2023-04-25T09:44:59.820710Z","shell.execute_reply.started":"2023-04-25T09:44:59.753902Z","shell.execute_reply":"2023-04-25T09:44:59.819446Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"params = {'train_file' : data_dir+'augmented_train_data.csv',\n          'dev_file' : data_dir+'val.csv',\n          'test_file' : data_dir+'test.csv',\n          'max_len' : 128,\n          'gpus' : 1,\n          'train_batch_size' : 4,\n          'eval_batch_size' : 2,\n          'epochs' : 50,\n          'label_smooth' : 0.1,\n          'src_seq_len' : 128,\n          'tgt_seq_len' : 32,\n          'num_layers' : 12,\n          'optimizer_lr' : 1e-4,\n          'optimizer_wd' : 1e-8,\n          'optimizer_eps' : 1e-8,\n          'scheduler_patience' : 1,\n          'embedding_dim' : 768,\n          'num_heads' : 12,\n          'dropout' : 0.1,\n          'fcn_hidden_dim' : 3072\n         }","metadata":{"execution":{"iopub.status.busy":"2023-04-25T09:44:59.822160Z","iopub.execute_input":"2023-04-25T09:44:59.823095Z","iopub.status.idle":"2023-04-25T09:44:59.834257Z","shell.execute_reply.started":"2023-04-25T09:44:59.823054Z","shell.execute_reply":"2023-04-25T09:44:59.833247Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"free_gpu_cache(stats = True)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T09:44:59.835666Z","iopub.execute_input":"2023-04-25T09:44:59.836086Z","iopub.status.idle":"2023-04-25T09:44:59.842859Z","shell.execute_reply.started":"2023-04-25T09:44:59.836040Z","shell.execute_reply":"2023-04-25T09:44:59.841865Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"asans = ASansRoBERTa(params).to(device)\ngpu_mem()\n#print_model_status(asans)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-04-25T09:44:59.844143Z","iopub.execute_input":"2023-04-25T09:44:59.844536Z","iopub.status.idle":"2023-04-25T09:45:03.432428Z","shell.execute_reply.started":"2023-04-25T09:44:59.844504Z","shell.execute_reply":"2023-04-25T09:45:03.430997Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Loading pre embedded data!\nLoading pre embedded data!\nFalse\n","output_type":"stream"}]},{"cell_type":"code","source":"#out = asans.train(baseline_checkpoint_path = data_dir)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T09:45:03.434520Z","iopub.execute_input":"2023-04-25T09:45:03.435275Z","iopub.status.idle":"2023-04-25T09:45:03.441408Z","shell.execute_reply.started":"2023-04-25T09:45:03.435221Z","shell.execute_reply":"2023-04-25T09:45:03.440089Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# out = asans.train(pre_checkpoint_path = save_dir, flag = True)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T09:45:03.442774Z","iopub.execute_input":"2023-04-25T09:45:03.443647Z","iopub.status.idle":"2023-04-25T09:45:03.450821Z","shell.execute_reply.started":"2023-04-25T09:45:03.443610Z","shell.execute_reply":"2023-04-25T09:45:03.449735Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"<h1><a href = './predictions.txt'>Download</a></h1>","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check(ckp_path):\n    score = 0\n    #free_gpu_cache(stats = True)\n    #data = asans.load_train_data(data_dir+'data.csv', save_dir+'new_data.pt', is_test = False)\n    batch = next(iter(asans.my_dataloader(asans.load_train_data(data_dir+'data.csv', save_dir+'new_data.pt', is_test = False), 10)))\n    srci, srcm, tgti, tgtg, tgtm = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device), batch[4].to(device)\n    checkpoint = torch.load(ckp_path, map_location = 'cpu')\n    #print(checkpoint['epoch'])\n    mdl = asans.get_params('model')\n    mdl.load_state_dict(checkpoint['model_state_dict'], strict = False);\n    out = mdl(srci, tgti, srcm, tgtm)\n    x = out.topk(1)[1].squeeze()\n    ground = decode(tgtg, back = True)\n    preds = decode(x, back = True)\n    orig = decode(srci, back = True)\n    for i in range(len(ground)):\n        score_prev = asans.scorer.get_scores(preds[i], ground[i], avg = True)\n        score += score_prev['rouge-1']['f']\n    df = pd.DataFrame(columns = ['original_text', 'predicted_summary', 'ground_summary'])\n    for i in range(len(ground)):\n        df.loc[len(df)] = {'original_text': orig[i],'predicted_summary': preds[i], 'ground_summary': ground[i]}\n    print(score)\n    return df\n    #df.to_csv('/kaggle/working/preds.csv')\n    #return download('preds.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-25T09:45:41.148025Z","iopub.execute_input":"2023-04-25T09:45:41.148662Z","iopub.status.idle":"2023-04-25T09:45:41.166535Z","shell.execute_reply.started":"2023-04-25T09:45:41.148599Z","shell.execute_reply":"2023-04-25T09:45:41.163450Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"df = check(ckp_path = '/kaggle/working/pgn_model_checkpoint1.pt')\ndf.to_csv('bert_pgn_predictions.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2023-04-25T09:45:41.507745Z","iopub.execute_input":"2023-04-25T09:45:41.508351Z","iopub.status.idle":"2023-04-25T09:45:52.309315Z","shell.execute_reply.started":"2023-04-25T09:45:41.508294Z","shell.execute_reply":"2023-04-25T09:45:52.307747Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Loading pre embedded data!\n1.600995768967815\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                       original_text  \\\n0  [CLS] वरषाकाल सखकरः भवति । असमिन काल विशषतया क...   \n1  [CLS] असमाक दशः भारतवरषम असति । अय हि हिमालयात...   \n2  [CLS] असमाक दश सरवास नदीष गगा अतिशरषठा परधाना ...   \n3  [CLS] सताम आचारः सदाचारः कथयत । सजजनाः यानि कर...   \n4  [CLS] यसमिन दश वय जनमधारण करमः स हि असमाक दशः ...   \n5  [CLS] पराचीनयगात असमाक समाज सतरीणा विशिषट सथान...   \n6  [CLS] शासनमन अनशासनम अरथात शासनन निरमितानि निय...   \n7  [CLS] यतर विविधानि पसतकानि पठनारथ सगहीतानि भवन...   \n8  [CLS] अय असमाक विदयालयः असति । असय भवनानि भवया...   \n9  [CLS] अधययनसय सरवथा परिवरतन जातम तथापि यावत छा...   \n\n                                   predicted_summary  \\\n0  । । । । । [SEP] । । । [SEP] । । । । । [SEP] । ...   \n1  ,,,,,, [SEP],,,,,,,,,,,,,,, [SEP],,,,,, [SEP] ...   \n2  । । । । । । । । । । । । । [SEP] । । । । । । । ...   \n3  । । । । । । [SEP] । । । । । । । । । । । । । [S...   \n4  । । । । । । । । । । । । । । । [SEP] । । । । । ...   \n5  । । । । । । । । । । [SEP] । । । । । । । । । । ...   \n6  । । । । । । । । । । । । । । । । । । [SEP] । । ...   \n7  । । । । । । । । । । । । । । [SEP] । । । । । । ...   \n8  । । । । । [SEP] । । । । । । । । । । । । । । [S...   \n9  । । । । । । । । । । । । । । । । । । । । [SEP] ...   \n\n                                      ground_summary  \n0  वरषाकाल सखकरः भवति । त कषिकारय करवनति । नभः मघ...  \n1  असमाक दशः भारतवरषम असति । अतर गगा, यमना, गोदाव...  \n2  असमाक दश सरवास नदीष गगा अतिशरषठा परधाना पवितरत...  \n3  सताम आचारः सदाचारः कथयत । ऋषयशच वदनति यानि अनि...  \n4  यसमिन दश वय जनमधारण करमः स हि असमाक दशः जनमभमि...  \n5  पराचीनयगात असमाक समाज सतरीणा विशिषट सथान वरतत ...  \n6  शासनमन अनशासनम अरथात शासनन निरमितानि नियमनि पा...  \n7  यतर विविधानि पसतकानि पठनारथ सगहीतानि भवनति तत ...  \n8  अय असमाक विदयालयः असति । असय परधानाधयापक [UNK]...  \n9  अधययनसय सरवथा परिवरतन जातम तथापि यावत छातरः छा...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original_text</th>\n      <th>predicted_summary</th>\n      <th>ground_summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS] वरषाकाल सखकरः भवति । असमिन काल विशषतया क...</td>\n      <td>। । । । । [SEP] । । । [SEP] । । । । । [SEP] । ...</td>\n      <td>वरषाकाल सखकरः भवति । त कषिकारय करवनति । नभः मघ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[CLS] असमाक दशः भारतवरषम असति । अय हि हिमालयात...</td>\n      <td>,,,,,, [SEP],,,,,,,,,,,,,,, [SEP],,,,,, [SEP] ...</td>\n      <td>असमाक दशः भारतवरषम असति । अतर गगा, यमना, गोदाव...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[CLS] असमाक दश सरवास नदीष गगा अतिशरषठा परधाना ...</td>\n      <td>। । । । । । । । । । । । । [SEP] । । । । । । । ...</td>\n      <td>असमाक दश सरवास नदीष गगा अतिशरषठा परधाना पवितरत...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[CLS] सताम आचारः सदाचारः कथयत । सजजनाः यानि कर...</td>\n      <td>। । । । । । [SEP] । । । । । । । । । । । । । [S...</td>\n      <td>सताम आचारः सदाचारः कथयत । ऋषयशच वदनति यानि अनि...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[CLS] यसमिन दश वय जनमधारण करमः स हि असमाक दशः ...</td>\n      <td>। । । । । । । । । । । । । । । [SEP] । । । । । ...</td>\n      <td>यसमिन दश वय जनमधारण करमः स हि असमाक दशः जनमभमि...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>[CLS] पराचीनयगात असमाक समाज सतरीणा विशिषट सथान...</td>\n      <td>। । । । । । । । । । [SEP] । । । । । । । । । । ...</td>\n      <td>पराचीनयगात असमाक समाज सतरीणा विशिषट सथान वरतत ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>[CLS] शासनमन अनशासनम अरथात शासनन निरमितानि निय...</td>\n      <td>। । । । । । । । । । । । । । । । । । [SEP] । । ...</td>\n      <td>शासनमन अनशासनम अरथात शासनन निरमितानि नियमनि पा...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>[CLS] यतर विविधानि पसतकानि पठनारथ सगहीतानि भवन...</td>\n      <td>। । । । । । । । । । । । । । [SEP] । । । । । । ...</td>\n      <td>यतर विविधानि पसतकानि पठनारथ सगहीतानि भवनति तत ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>[CLS] अय असमाक विदयालयः असति । असय भवनानि भवया...</td>\n      <td>। । । । । [SEP] । । । । । । । । । । । । । । [S...</td>\n      <td>अय असमाक विदयालयः असति । असय परधानाधयापक [UNK]...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>[CLS] अधययनसय सरवथा परिवरतन जातम तथापि यावत छा...</td>\n      <td>। । । । । । । । । । । । । । । । । । । । [SEP] ...</td>\n      <td>अधययनसय सरवथा परिवरतन जातम तथापि यावत छातरः छा...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.loc[1]","metadata":{"execution":{"iopub.status.busy":"2023-04-25T09:53:23.008302Z","iopub.execute_input":"2023-04-25T09:53:23.008805Z","iopub.status.idle":"2023-04-25T09:53:23.018471Z","shell.execute_reply.started":"2023-04-25T09:53:23.008759Z","shell.execute_reply":"2023-04-25T09:53:23.017093Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"original_text        [CLS] असमाक दशः भारतवरषम असति । अय हि हिमालयात...\npredicted_summary    ,,,,,, [SEP],,,,,,,,,,,,,,, [SEP],,,,,, [SEP] ...\nground_summary       असमाक दशः भारतवरषम असति । अतर गगा, यमना, गोदाव...\nName: 1, dtype: object"},"metadata":{}}]}]}